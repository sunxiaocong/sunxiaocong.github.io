---
layout: post
title: "PySpark 笔记(上)"
date: 2019-01-22
categories: 大数据
tags: [Spark,Python]
image: http://gastonsanchez.com/images/blog/mathjax_logo.png
---

PySpark 是 Spark 为 Python 开发者提供的 API [1]  ，位于 $SPARK_HOME/bin 目录
本文主要为记录学习过程
<!-- more -->

## 1. PySpark 简介

### 1.1 Spark集群模式
* Standalone  Master/Slave 的主从模式 
* Yarn 基于Yarn集群管理 由Yarn负责集群调度
* Mesos 
* Kubernetes

### 1.2 Spark Core

* 他涵盖了Spark基础功能，包括任务分配、内存管理、故障恢复与整个系统相互作用。
    他为分布式数据集 RDD 提供  API 接口

### 1.3 SparkSQL

* 提供结构化的数据加载与存储，让用户可以用类似SQL的语句对数据进行查询

### 1.4 SparkStreaming

* 用于处理流数据。可以自定义窗口大小计算时间范围内的数据

### 1.5 MLlib

* 为机器学习提供API接口

### 1.6 GraphX

* Spark GraphX是一个分布式图处理框架，它是基于Spark平台提供对图计算和图挖掘简洁易用的而丰富的接口，极大的方便了对分布式图处理的需求。

## 2. Spark初始化

### 2.1 SparkConf

#### 2.1.1 setMaster 

集群地址

```yaml
spark://host:port   Standalone clusterport 7077.
mesos://host:port   Mesos cluster.port 5050.
yarn  YARN cluster. 需要设置 HADOOP_CONF_DIR
local 本地单核
local[N] Run in local mode with N cores.
local[*] Run in local mode and use as many cores as the machine has.
```

#### 2.1.2 spark.cassandra.connection.host 
cassandra地址
#### 2.1.3 setAppName()
APP名称
#### 2.1.4 setSparkHome()
#### 2.1.5 setExecutorEnv()

```ipython
>>> conf.setExecutorEnv("VAR1", "value1")
<pyspark.conf.SparkConf object at ...>
>>> conf.setExecutorEnv(pairs = [("VAR3", "value3"), ("VAR4", "value4")])
<pyspark.conf.SparkConf object at ...>
>>> conf.get("spark.executorEnv.VAR1")
u'value1'
>>> print(conf.toDebugString())
spark.executorEnv.VAR1=value1
spark.executorEnv.VAR3=value3
spark.executorEnv.VAR4=value4
```

#### 2.1.6 spark.executor.memory 
每个execute内存
#### 2.1.7 spark.driver.memory
driver内存
#### 2.1.8 spark.cores.max 

分配核心数

### 2.2  spark-submit
#### 2.2.1 --master 
Indicates the cluster manager to connect to. The options for this flag are described in Table 7-1.
#### 2.2.2  --deploy-mode 
Whether to launch the driver program locally (“client”) or on one of the worker machines inside the
cluster (“cluster”). In client mode spark-submit will run your driver on the same machine where
spark-submit is itself being invoked. In cluster mode, the driver will be shipped to execute on a
worker node in the cluster. The default is client mode.
#### 2.2.3 --class 
The “main” class of your application if you’re running a Java or Scala program.
#### 2.2.4 --name 
A human-readable name for your application. This will be displayed in Spark’s web UI.
#### 2.2.5 --jars 
A list of JAR files to upload and place on the classpath of your application. If your application depends
on a small number of third-party JARs, you can add them here.
#### 2.2.6 --files 
A list of files to be placed in the working directory of your application. This can be used for data files
that you want to distribute to each node.
#### 2.2.7 --py-files 
A list of files to be added to the PYTHONPATH of your application. This can contain .py, .egg, or .zip
files.