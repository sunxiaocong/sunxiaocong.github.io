---
layout: post
title: "PySpark 笔记"
date: 2019-01-22
categories: 大数据
tags: [Spark,Python]
image: http://gastonsanchez.com/images/blog/mathjax_logo.png
---

PySpark 是 Spark 为 Python 开发者提供的 API [1]  ，位于 $SPARK_HOME/bin 目录
本文主要为记录学习过程
<!-- more -->

## 1. PySpark 简介

### 1.1 Spark集群模式
* Standalone  Master/Slave 的主从模式 
* Yarn 基于Yarn集群管理 由Yarn负责集群调度
* Mesos 
* Kubernetes

### 1.2 Spark Core

* 他涵盖了Spark基础功能，包括任务分配、内存管理、故障恢复与整个系统相互作用。
    他为分布式数据集 RDD 提供  API 接口

### 1.3 SparkSQL

* 提供结构化的数据加载与存储，让用户可以用类似SQL的语句对数据进行查询

### 1.4 SparkStreaming

* 用于处理流数据。可以自定义窗口大小计算时间范围内的数据

### 1.5 MLlib

* 为机器学习提供API接口

### 1.6 GraphX

* Spark GraphX是一个分布式图处理框架，它是基于Spark平台提供对图计算和图挖掘简洁易用的而丰富的接口，极大的方便了对分布式图处理的需求。

## 2. Spark初始化

### 2.1 SparkConf

#### 2.1.1 setMaster 

集群地址

```yaml
spark://host:port   Standalone clusterport 7077.
mesos://host:port   Mesos cluster.port 5050.
yarn  YARN cluster. 需要设置 HADOOP_CONF_DIR
local 本地单核
local[N] Run in local mode with N cores.
local[*] Run in local mode and use as many cores as the machine has.
```

#### 2.1.2 spark.cassandra.connection.host 
cassandra地址
#### 2.1.3 setAppName()
APP名称
#### 2.1.4 setSparkHome()
#### 2.1.5 setExecutorEnv()

```ipython
>>> conf.setExecutorEnv("VAR1", "value1")
<pyspark.conf.SparkConf object at ...>
>>> conf.setExecutorEnv(pairs = [("VAR3", "value3"), ("VAR4", "value4")])
<pyspark.conf.SparkConf object at ...>
>>> conf.get("spark.executorEnv.VAR1")
u'value1'
>>> print(conf.toDebugString())
spark.executorEnv.VAR1=value1
spark.executorEnv.VAR3=value3
spark.executorEnv.VAR4=value4
```

#### 2.1.6 spark.executor.memory 
每个execute内存
#### 2.1.7 spark.driver.memory
driver内存
#### 2.1.8 spark.cores.max 

分配核心数

### 2.2  spark-submit
#### 2.2.1 --master 
Indicates the cluster manager to connect to. The options for this flag are described in Table 7-1.
#### 2.2.2  --deploy-mode 
Whether to launch the driver program locally (“client”) or on one of the worker machines inside the
cluster (“cluster”). In client mode spark-submit will run your driver on the same machine where
spark-submit is itself being invoked. In cluster mode, the driver will be shipped to execute on a
worker node in the cluster. The default is client mode.
#### 2.2.3 --class 
The “main” class of your application if you’re running a Java or Scala program.
#### 2.2.4 --name 
A human-readable name for your application. This will be displayed in Spark’s web UI.
#### 2.2.5 --jars 
A list of JAR files to upload and place on the classpath of your application. If your application depends
on a small number of third-party JARs, you can add them here.
#### 2.2.6 --files 
A list of files to be placed in the working directory of your application. This can be used for data files
that you want to distribute to each node.
#### 2.2.7 --py-files 
A list of files to be added to the PYTHONPATH of your application. This can contain .py, .egg, or .zip
files.

## 3. RDD分布式数据集使用

### 3.1 RDD简介
* RDD（Resilient Distributed Dataset）叫做弹性分布式数据集，
是Spark中最基本的数据抽象，它代表一个不可变、可分区、里面的元素可并行计算的集合。R
    *   Partition，数据集的基本组成单位。对于RDD来说，每个分片都会被一个计算任务处理，
    并决定并行计算的粒度。RDD的分配个数默认为CPU核心数。 (rdd.partitionBy(20, hash_domain) # Create 20 partitions)

    * Spark中RDD的计算是以分片为单位的，每个RDD都会实现compute函数并且对迭代器进行复合以达到这个目的。
    
    * RDD的每次转换都会生成一个新的RDD，所以RDD之间就会形成类似于流水线一样的前后依赖关系。
    在部分分区数据丢失时，Spark可以通过这个依赖关系重新计算丢失的分区数据，而不是对RDD的所有分区进行重新计算。

    * 当前Spark中实现了两种类型的分片函数，一个是基于哈希的HashPartitioner，
    另外一个是基于范围的RangePartitioner。只有对于于key-value的RDD，才会有Partitioner，
    非key-value的RDD的Parititioner的值是None。Partitioner函数不但决定了RDD本身的分片数量，也决定了parent RDD 
    Shuffle输出时的分片数量。

    * 对于一个HDFS文件来说，这个列表保存的就是每个Partition所在的块的位置。
    按照“移动数据不如移动计算”的理念，Spark在进行任务调度的时候，
    会尽可能地将计算任务分配到其所要处理数据块的存储位置。
    
    
### 3.2 RDD编程(python)

#### 3.2.1 RDD Basics
```python
# 读取本地文件内容
lines = sc.textFile("README.md")
# 读取HDFS文件内容
lines = sc.textFile("hdfs://xxxx/xx/xxx")
# 根据数组写入Spark
lines = sc.parallelize(["pandas", "i like pandas"])
```

#### 3.2.1 map()
```python
# 参数是函数，函数应用于RDD每一个元素，返回值是新的RDD 
newRDD = RDD.map(lambda x:x[0])

# RDD --> ["123","123","123","123","123"]
#newRDD -> ["1","1","1","1","1"]

# 其中匿名函数 lamdbda 等价于
def func(x):
    return x[0]
newRDD = RDD.map(func)
```

#### 3.2.2 flatMap()
```python
# 参数是函数，函数应用于RDD每一个元素，将元素数据进行拆分，变成迭代器，返回值是新的RDD
newRDD = RDD.flatMap(lambda x:x.split("2"))

# RDD --> ["123","123"]
# newRDD -> ["1","3","1","3"]
```    

#### 3.2.3 filter()
```python
# 参数是函数，函数会过滤掉不符合条件的元素，返回值是新的RDD
newRDD = RDD.filter(lambda x: "12" in x )

# RDD --> ["12","123","234"]
# newRDD -> ["12","123"]
```

#### 3.2.4 distinct()
```python
# 没有参数，将RDD里的元素进行去重操作
newRDD = RDD.distinct()

# RDD --> ["12","12","234"]
# newRDD -> ["12","234"]
```

#### 3.2.5 union()
```python
# 参数是RDD，生成包含两个RDD 所有 元素的新RDD
newRDD = RDD.distinct(oldRDD)

# oldRDD --> ["12","12","234"]
# RDD --> ["12","12","234"]
# newRDD -> ["12","12","234","12","12","234"]
```

#### 3.2.6 intersection()
```python
# 参数是RDD，求出两个RDD的共同元素
newRDD = RDD.intersection(oldRDD)

# oldRDD --> ["12","12","234","2234"]
# RDD --> ["12","12","234","5687"]
# newRDD --> ["12","12","234"]
```

#### 3.2.7 subtract()
```python
# 参数是RDD，将原RDD里和参数RDD里相同的元素去掉
newRDD = RDD.subtract(oldRDD)

# oldRDD --> ["12","12","234","2234"]
# RDD --> ["12","12","234","5687"]
# newRDD --> ["2234","5687"]
```

#### 3.2.8 cartesian()
```python
# 参数是RDD，求两个RDD的笛卡儿积
newRDD = RDD.cartesian(oldRDD)

# RDD --> [a, b]
#　oldRDD --> [0, 1, 2]
# newRDD --> [(a, 0), (a, 1), (a, 2), (b, 0), (b, 1), (b, 2)]
```

#### 3.2.9 action
* collect(),返回RDD所有元素
* count(),RDD里元素个数
* top(num),取最前几个
* take(num),取最后几个
* takeOrdered(num)(ordering)，取排序后的前几个
* countByValue()，各元素在RDD中出现次数
* reduce()

```python
# 并行整合所有RDD数据，例如连乘操作
# 参数是函数

newRDD = RDD.reduce(lambda x , y : x * y )

# RDD --> [1,2,3,4,5]
# newRDD --> 120
```
* fold(0)(func)，和reduce功能一样，不过fold带有初始值
* aggregate(0)(seqOp,combop)，和reduce功能一样，但是返回的RDD数据类型和原RDD不一样
* foreach(func)对RDD每个元素都是使用特定函数

### 3.3 PairRDDs(key/value)

Transformations on one pair RDD

#### 3.3.1 reduceByKey(func) 
```python
# 并行整合每一个key的数据 例如将每一key的累加和
newRDD = RDD.reduceByKey(lambda x , y : x * y )

# RDD --> {(1, 2), (3, 4), (3, 6)}
# nweRDD --> {(1,2), (3,10)}

```
#### 3.3.2 groupByKey()
```python
# 根据key进行分组
nweRDD = RDD.groupByKey()

# RDD --> {(1, 2), (3, 4), (3, 6)}
#　nweRDD -->{(1,[2]),(3, [4,6])}
```

#### 3.3.3 mapValues(func)
```python
# 分别对每一个key 进行map 在组合在一起
newRDD = RDD.mapValues(lambda x:x+1)

# RDD --> {(1, 2), (3, 4), (3, 6)}
# nweRDD --> {(1,3), (3,5), (3,7)}
```

#### 3.3.4 flatMapValues(func)
```
    同上 + flatmap
```
#### 3.3.5 keys()
```python
newRDD = RDD.keys() 

# RDD --> {(1, 2), (3, 4), (3, 6)}
# newRDD --> {1, 3,3}
```

#### 3.3.6 value()
```python
newRDD = RDD.value() 

# RDD --> {(1, 2), (3, 4), (3, 6)}
# newRDD --> {2, 4 , 6}        
```

#### 3.3.7 sortByKey()
```python
# 根据key值排序
newRDD = RDD.sortByKey() 

# RDD --> {(1, 2), (3, 4), (3, 6)}
#　newRDD -->{(1,2), (3,4), (3,6)}
```

### 3.4 Transformations on two pair RDD

#### 3.4.1 subtractByKey(other : RDD)
```python
# 根据移除RDD中 oldRDDKey 的值相同的 
newRDD = RDD.subtractByKey(oldRDD)

#RDD --> {(1, 2), (3, 4), (3, 6)}
#oldRDD --> {(3, 9)}
#newRDD --> {(1, 2)}
```

#### 3.4.2 join(other : RDD)
```python
#执行内部连接 和sql链接操作类似
newRDD = RDD.join(oldRDD)

#RDD --> {(1, 2), (3, 4), (3, 6)}
#oldRDD --> {(3, 9)}
#newRDD --> {(3, (4, 9)), (3,(6, 9))}
```

#### 3.4.3 rightOuterJoin
```python
# 右边链接左边 some在左
newRDD = RDD.rightOuterJoin(oldRDD)

# RDD --> {(1, 2), (3, 4), (3, 6)}
# oldRDD --> {(3, 9)}
#　newRDD --> {(3,(Some(4),9)),(3,(Some(6),9))}
```

####3.4.4 leftOuterJoin
```python
# 左边链接右边 some在右
newRDD = RDD.rightOuterJoin(oldRDD)

# RDD --> {(1, 2), (3, 4), (3, 6)}
#oldRDD --> {(3, 9)}
#　newRDD --> {(1,(2,None)), (3,(4,Some(9))), (3,(6,Some(9)))}
```

#### 3.4.5 cogroup
```python
# 将两个rdd进行组合 共享key值
newRDD = RDD.rightOuterJoin(oldRDD)

# RDD --> {(1, 2), (3, 4), (3, 6)}
# oldRDD --> {(3, 9)}
# newRDD --> {(1,([2],[])), (3,([4, 6],[9]))}
```

### 3.5 Actions Available on Pair RDDs

#### 3.5.1 countByKey()

#### 3.5.2 collectAsMap()
```python
map = RDD.collectAsMap()

# RDD --> {(1, 2), (3, 4), (3, 6)}
# map --> Map{(1, 2), (3,4), (3, 6)}
```

#### 3.5.3 lookup(key)
```python
# 返回 key 对应的所有value
values = RDD.lookup(3)

# RDD --> {(1, 2), (3, 4), (3, 6)}
# values --> [4,6]
```


