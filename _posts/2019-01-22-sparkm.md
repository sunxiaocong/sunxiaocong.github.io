---
layout: post
title: "PySpark 笔记(中)"
date: 2019-01-22
categories: 大数据
tags: [Spark,Python]
image: http://gastonsanchez.com/images/blog/mathjax_logo.png
---

PySpark 是 Spark 为 Python 开发者提供的 API [1]  ，位于 $SPARK_HOME/bin 目录
本文主要为记录学习过程
<!-- more -->

## 3. RDD分布式数据集使用

### 3.1 RDD简介
* RDD（Resilient Distributed Dataset）叫做弹性分布式数据集，
是Spark中最基本的数据抽象，它代表一个不可变、可分区、里面的元素可并行计算的集合。R
    *   Partition，数据集的基本组成单位。对于RDD来说，每个分片都会被一个计算任务处理，
    并决定并行计算的粒度。RDD的分配个数默认为CPU核心数。 (rdd.partitionBy(20, hash_domain) # Create 20 partitions)

    * Spark中RDD的计算是以分片为单位的，每个RDD都会实现compute函数并且对迭代器进行复合以达到这个目的。
    
    * RDD的每次转换都会生成一个新的RDD，所以RDD之间就会形成类似于流水线一样的前后依赖关系。
    在部分分区数据丢失时，Spark可以通过这个依赖关系重新计算丢失的分区数据，而不是对RDD的所有分区进行重新计算。

    * 当前Spark中实现了两种类型的分片函数，一个是基于哈希的HashPartitioner，
    另外一个是基于范围的RangePartitioner。只有对于于key-value的RDD，才会有Partitioner，
    非key-value的RDD的Parititioner的值是None。Partitioner函数不但决定了RDD本身的分片数量，也决定了parent RDD 
    Shuffle输出时的分片数量。

    * 对于一个HDFS文件来说，这个列表保存的就是每个Partition所在的块的位置。
    按照“移动数据不如移动计算”的理念，Spark在进行任务调度的时候，
    会尽可能地将计算任务分配到其所要处理数据块的存储位置。
    
    
### 3.2 RDD编程(python)

#### 3.2.1 RDD Basics
```python
# 读取本地文件内容
lines = sc.textFile("README.md")
# 读取HDFS文件内容
lines = sc.textFile("hdfs://xxxx/xx/xxx")
# 根据数组写入Spark
lines = sc.parallelize(["pandas", "i like pandas"])
```

#### 3.2.1 map()
```python
# 参数是函数，函数应用于RDD每一个元素，返回值是新的RDD 
newRDD = RDD.map(lambda x:x[0])

# RDD --> ["123","123","123","123","123"]
#newRDD -> ["1","1","1","1","1"]

# 其中匿名函数 lamdbda 等价于
def func(x):
    return x[0]
newRDD = RDD.map(func)
```

#### 3.2.2 flatMap()
```python
# 参数是函数，函数应用于RDD每一个元素，将元素数据进行拆分，变成迭代器，返回值是新的RDD
newRDD = RDD.flatMap(lambda x:x.split("2"))

# RDD --> ["123","123"]
# newRDD -> ["1","3","1","3"]
```    

#### 3.2.3 filter()
```python
# 参数是函数，函数会过滤掉不符合条件的元素，返回值是新的RDD
newRDD = RDD.filter(lambda x: "12" in x )

# RDD --> ["12","123","234"]
# newRDD -> ["12","123"]
```

#### 3.2.4 distinct()
```python
# 没有参数，将RDD里的元素进行去重操作
newRDD = RDD.distinct()

# RDD --> ["12","12","234"]
# newRDD -> ["12","234"]
```

#### 3.2.5 union()
```python
# 参数是RDD，生成包含两个RDD 所有 元素的新RDD
newRDD = RDD.distinct(oldRDD)

# oldRDD --> ["12","12","234"]
# RDD --> ["12","12","234"]
# newRDD -> ["12","12","234","12","12","234"]
```

#### 3.2.6 intersection()
```python
# 参数是RDD，求出两个RDD的共同元素
newRDD = RDD.intersection(oldRDD)

# oldRDD --> ["12","12","234","2234"]
# RDD --> ["12","12","234","5687"]
# newRDD --> ["12","12","234"]
```

#### 3.2.7 subtract()
```python
# 参数是RDD，将原RDD里和参数RDD里相同的元素去掉
newRDD = RDD.subtract(oldRDD)

# oldRDD --> ["12","12","234","2234"]
# RDD --> ["12","12","234","5687"]
# newRDD --> ["2234","5687"]
```

#### 3.2.8 cartesian()
```python
# 参数是RDD，求两个RDD的笛卡儿积
newRDD = RDD.cartesian(oldRDD)

# RDD --> [a, b]
#　oldRDD --> [0, 1, 2]
# newRDD --> [(a, 0), (a, 1), (a, 2), (b, 0), (b, 1), (b, 2)]
```

#### 3.2.9 action

### 3.3 PairRDDs(key/value)

#### 3.3.1 reduceByKey(func) 
```python
# 并行整合每一个key的数据 例如将每一key的累加和
newRDD = RDD.reduceByKey(lambda x , y : x * y )

# RDD --> {(1, 2), (3, 4), (3, 6)}
# nweRDD --> {(1,2), (3,10)}

```
#### 3.3.2 groupByKey()
```python
# 根据key进行分组
nweRDD = RDD.groupByKey()

# RDD --> {(1, 2), (3, 4), (3, 6)}
#　nweRDD -->{(1,[2]),(3, [4,6])}
```

#### 3.3.3 mapValues(func)
```python
# 分别对每一个key 进行map 在组合在一起
newRDD = RDD.mapValues(lambda x:x+1)

# RDD --> {(1, 2), (3, 4), (3, 6)}
# nweRDD --> {(1,3), (3,5), (3,7)}
```

#### 3.3.4 flatMapValues(func)
```
    同上 + flatmap
```

#### 3.3.5 keys()
```python
newRDD = RDD.keys() 

# RDD --> {(1, 2), (3, 4), (3, 6)}
# newRDD --> {1, 3,3}
```

#### 3.3.6 value()
```python
newRDD = RDD.value() 

# RDD --> {(1, 2), (3, 4), (3, 6)}
# newRDD --> {2, 4 , 6}        
```

#### 3.3.7 sortByKey()
```python
# 根据key值排序
newRDD = RDD.sortByKey() 

# RDD --> {(1, 2), (3, 4), (3, 6)}
#　newRDD -->{(1,2), (3,4), (3,6)}
```

### 3.4 Transformations on two pair RDD

#### 3.4.1 subtractByKey(other : RDD)
```python
# 根据移除RDD中 oldRDDKey 的值相同的 
newRDD = RDD.subtractByKey(oldRDD)

#RDD --> {(1, 2), (3, 4), (3, 6)}
#oldRDD --> {(3, 9)}
#newRDD --> {(1, 2)}
```

#### 3.4.2 join(other : RDD)
```python
#执行内部连接 和sql链接操作类似
newRDD = RDD.join(oldRDD)

#RDD --> {(1, 2), (3, 4), (3, 6)}
#oldRDD --> {(3, 9)}
#newRDD --> {(3, (4, 9)), (3,(6, 9))}
```

#### 3.4.3 rightOuterJoin
```python
# 右边链接左边 some在左
newRDD = RDD.rightOuterJoin(oldRDD)

# RDD --> {(1, 2), (3, 4), (3, 6)}
# oldRDD --> {(3, 9)}
#　newRDD --> {(3,(Some(4),9)),(3,(Some(6),9))}
```

####3.4.4 leftOuterJoin
```python
# 左边链接右边 some在右
newRDD = RDD.rightOuterJoin(oldRDD)

# RDD --> {(1, 2), (3, 4), (3, 6)}
#oldRDD --> {(3, 9)}
#　newRDD --> {(1,(2,None)), (3,(4,Some(9))), (3,(6,Some(9)))}
```

#### 3.4.5 cogroup
```python
# 将两个rdd进行组合 共享key值
newRDD = RDD.rightOuterJoin(oldRDD)

# RDD --> {(1, 2), (3, 4), (3, 6)}
# oldRDD --> {(3, 9)}
# newRDD --> {(1,([2],[])), (3,([4, 6],[9]))}
```

### 3.5 Actions Available on Pair RDDs

#### 3.5.1 countByKey()

#### 3.5.2 collectAsMap()
```python
map = RDD.collectAsMap()

# RDD --> {(1, 2), (3, 4), (3, 6)}
# map --> Map{(1, 2), (3,4), (3, 6)}
```

#### 3.5.3 lookup(key)
```python
# 返回 key 对应的所有value
values = RDD.lookup(3)

# RDD --> {(1, 2), (3, 4), (3, 6)}
# values --> [4,6]
```

