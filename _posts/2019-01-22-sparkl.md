---
layout: post
title: "PySpark 笔记(下)"
date: 2019-01-22
categories: 大数据
tags: [Spark,Python]
image: http://gastonsanchez.com/images/blog/mathjax_logo.png
---

PySpark 是 Spark 为 Python 开发者提供的 API [1]  ，位于 $SPARK_HOME/bin 目录
本文主要 介绍 Pyspark 对于数据的加载与SQL操作
<!-- more -->

## 4. Load and Save

### 4.1 text files
- 无结构
- 每行为一条记录
- Load
```python
input = sc.textFile("file:///home/holden/repos/spark/README.md")
"s3n://bucket/my-files/*.txt"
"hdfs://master:port/path"
```
- Save
```python
result.saveAsTextFile(outputFile)
```

### 4.2 Json
- 半结构化
- 大多数行为一个计入
- Load
```python
import json 
data = input.map(lambda x: json.loads(x))
data = input.jsonFile("tweets.json")
df = sqlContext.read \
    .format('json').load('py/test/sql/people.json')
df = sparksession.read.format('json').load('py/test/sql/people.json')
df = sparksession.read.json('py/test/sql/people.json')
```
- Save
```python
(data.filter(lambda x: x['lovesPandas']).map(lambda x: json.dumps(x))
                                           .saveAsTextFile(outputFile))
```

### 4.3 CSV
- 结构化
- 通常用于电子表格
- Load
```python
import csv
import StringIO

def loadRecord(line):
    """Parse a CSV line"""
    input = StringIO.StringIO(line)
    reader = csv.DictReader(input, fieldnames=["name", "favouriteAnimal"])
    return reader.next()
input = sc.textFile(inputFile).map(loadRecord)
def loadRecords(fileNameContents):
    """Load all the records in a given file"""
    input = StringIO.StringIO(fileNameContents[1])
    reader = csv.DictReader(input, fieldnames=["name", "favoriteAnimal"])
    return reader
fullFileData = sc.wholeTextFiles(inputFile).flatMap(loadRecords)
```
- Save
```python
def writeRecords(records):
    """Write out CSV lines"""
    output = StringIO.StringIO()
    writer = csv.DictWriter(output, fieldnames=["name", "favoriteAnimal"])
    for record in records:
        writer.writerow(record)
    return [output.getvalue()]
pandaLovers.mapPartitions(writeRecords).saveAsTextFile(outputFile)
```

### 4.4 SequenceFiles 
- 结构化
- 键值对类型的数据
- Load
```python
data = sc.sequenceFile(inFile,
    "org.apache.hadoop.io.Text", "org.apache.hadoop.io.IntWritable")
```

- Save
```python
data.saveAsSequenceFile(outputFile)
```
### 4.5 Protocol buffers
- 结构化
- 一种快速、节省空间的多语言格式。
### 4.6 Object files
- 结构化
- 在Spark作业中用于共享
- Load
```python
pickleFile()
```
- Save
```python
saveAsPickleFile()
```

## 5. SQL

### 5.1  Apache Hive
#### 5.1.1 加载数据与使用
~~~python
# Import Spark SQL
from pyspark.sql import HiveContext, Row
# Or if you can't include the hive requirements
from pyspark.sql import SQLContext, Row
hiveCtx = HiveContext(sc)
rows = hiveCtx.sql("SELECT name, age FROM users") # rows (SchemaRDDs)
firstRow = rows.first()
print firstRow.name

"""tweets
{"user": {"name": "Holden", "location": "San Francisco"}, "text": "Nice day out today"}
{"user": {"name": "Matei", "location": "Berkeley"}, "text": "Even nicer here :)"}
"""
tweets = hiveCtx.jsonFile("tweets.json")
tweets.registerTempTable("tweets") #注册成为临时表 提供sql查询
results = hiveCtx.sql("SELECT user.name, text FROM tweets")

results.show() #画图表

results.printSchema() #话出结构

results.count() #
~~~
#### 5.1.2 自定义结构
~~~python
schema = StructType([
StructField("id", LongType(), True),
StructField("name", StringType(), True),
StructField("age", LongType(), True),
StructField("eyeColor", StringType(), True)
])
~~~
~~~python
import pyspark.sql.types as typ
fraud = sc.textFile('ccFraud.csv.gz')
# 提取表头
header = fraud.first()
# 获取数据
fraud = fraud \
    .filter(lambda row: row != header) \
    .map(lambda row: [int(elem) for elem in row.split(',')])
 
使用typ够着结构
fields = [
    *[
    typ.StructField(h[1:-1], typ.IntegerType(), True)
    for h in header.split(',')
    ]
]
schema = typ.StructType(fields)

# 创建DataFrame
fraud_df = spark.createDataFrame(fraud, schema)

fraud_df.printSchema() # 打印表结构
~~~
### 5.2  Mongo
#### 5.2.1  数据加载与使用
~~~python
# mongo 加载数据　
from pyspark import SparkConf, SparkContext
from pyspark.sql import SQLContext
from pyspark.sql.types import *
sc = SparkContext()
ctx = SQLContext(sc)

test_collection = ctx.read.format("com.mongodb.spark.sql").options(uri="mongodb://192.168.0.1:27017",　database="test_db", collection="test_collection").load()  

#　不指定数据类型加载太慢　这里自定义类型
fields_list = "name age sex grade exp"  
fields = [StructField(field_name, StringType(), True) for field_name in fields_list.split()]  
schema = StructType(fields)  
test_collection = job_ctx.read.schema(schema).format("com.mongodb.spark.sql").options(uri="mongodb://192.168.0.1:27017", database="test_db",  collection="test_collection").load()  

# 注册成为临时表
test_collection.registerTempTable("Account")  
sql ＝ "select * from Account where age > '18'"  
result = ctx.sql(sql)  

# 若　result为rdd　格式需要先转换为　dataframe    
fields_list = "name age sex grade exp"  
fields = [StructField(field_name, StringType(), True) for field_name in fields_list]  
schema = StructType(fields)  
df = ctx.createDataFrame(result, schema=schema)  
df.write.format("com.mongodb.spark.sql").mode("overwrite").options(uri="mongodb://192.168.0.1:27017",  database="test_db", collection="test_collection_out").load()  


# overwrite就是先删除mongodb中指定的表，然后把数据写到这个表中；ignore就是如果mongodb中有这个表，就不写数据了，且不会报错；errorifexists就是如果mongodb中存在这个表就报错，如果不存在就正常写入；append就是不管mongodb中这个表存不存在直接往里写数据
~~~

### 5.3   DataFrame
#### 5.3.1   构造
~~~python
df = spark.createDataFrame([
     (1, 144.5, 5.9, 33, 'M'),
     (2, 167.2, 5.4, 45, 'M'),
     (3, 124.1, 5.2, 23, 'F'),
     (4, 144.5, 5.9, 33, 'M'),
     (5, 133.2, 5.7, 54, 'F'),
     (3, 124.1, 5.2, 23, 'F'),
     (5, 129.2, 5.3, 42, 'M'),
     ], ['id', 'weight', 'height', 'age', 'gender'])

~~~
#### 5.3.2 count
~~~python
print('Count of rows: {0}'.format(df.count()))
# >>> 7
print('Count of distinct rows: {0}'.format(df.distinct().count()))
# >>> 6
~~~
#### 5.3.3  show
~~~python
df.show() 
df.where('id == 3').show()  选出id = 3 的row 
sparksession.sql("select count(1) from swimmers").show() 

~~~
#### 5.3.4   filter
~~~python
df_outliers = df_outliers.join(outliers, on='id')
df_outliers.filter('weight_o').select('id', 'weight').show()
df_outliers.filter('age_o').select('id', 'age').show()
~~~
#### 5.3.5  select
~~~python
swimmers.select("name", "eyeColor").filter("eyeColor like 'b%'").show() 用filter进行筛选

# 输出的字段将没有income
df_miss_no_income = df_miss.select([
    c for c in df_miss.columns if c != 'income'
]).show 
~~~
#### 5.3.6  dropDuplicates
~~~python
#删除重复字段复制
df = df.dropDuplicates()

# 自定义条件的复制
df = df.dropDuplicates(subset=[
    c for c in df.columns if c != 'id'
]) 
~~~
        
#### 5.3.7  dropna
~~~python
# 当某一个字段超过三条计入有空 则删除该row 
df_miss_no_income.dropna(thresh=3).show()
~~~
#### 5.3.8   agg
~~~python
# 聚合操作 alias 为显示字段的命名
import pyspark.sql.functions as fn
df.agg(
     fn.count('id').alias('count'),
     fn.countDistinct('id').alias('distinct')
    ).show()
    
# 多个字段以生成式方式写出 计算每一个字段中少了多少
df_miss.agg(*[
    (1 - (fn.count(c) / fn.count('*'))).alias(c + '_missing')
    for c in df_miss.columns
]).show()
~~~
#### 5.3.9   map
~~~python
# 计算每行有几个None
df_miss.rdd.map(
    lambda row: (row['id'], sum([c == None for c in row]))
).collect()
~~~
#### 5.3.10  withColumn
~~~python
#添加一个字段
df.withColumn('new_id', fn.monotonically_increasing_id()).show()

~~~
#### 5.3.11   fillna
~~~python
# 用于估算空值的 这里使用 mean 平均数 先把每一个平均数给补上
# gender不是数字类型所以跳过

# to_dict 的计入在records内 
means = df_miss_no_income.agg(
    *[fn.mean(c).alias(c)
    for c in df_miss_no_income.columns if c != 'gender']
    ).toPandas().to_dict('records')[0]
# 补上gender的值
means['gender'] = 'missing'
# 对其中的空值进行赋值
df_miss_no_income.fillna(means).show()
~~~

### 5.4 SchemaRDDs
可以和RDD一样的操作
还可以有SQL的查询操作
~~~python
## rdd --> scemardd
happyPeopleSchemaRDD = hiveCtx.inferSchema(happyPeopleRDD)
happyPeopleSchemaRDD.registerTempTable("happy_people")
~~~
    
### 5.5  Types stored 

 | SparkSQL/HiveQL | Python | 
| ------ | ------ | 
| TINYINT | int/long (in range of –128 to 127) | 
| SMALLINT | int/long (in range of –32768 to 32767) | 
| INT | int or long | 
| BIGINT | long | 
| FLOAT | float | 
| DOUBLE | float | 
| DECIMAL | decimal.Decimal | 
| STRING | string | 
| BINARY | bytearray | 
| BOOLEAN | bool | 
| TIMESTAMP | datetime.datetime | 
| ARRAY<DATA_TYPE> | list, tuple, or array | 
| MAP<KEY_TYPE,VAL_TYPE> | dict | 
| STRUCT<COL1:COL1_TYPE, ...> | Row| 
 
### 5.6  UDFs （自定义查询返回）
~~~python
hiveCtx.registerFunction("strLenPython", lambda x: len(x), IntegerType())
lengthSchemaRDD = hiveCtx.sql("SELECT strLenPython('text') FROM tweets LIMIT 10")
~~~

### 5.7 Performance Tuning Options
*   spark.sql.codegen
    *   当这是真的时，Spark SQL会编译每个动态地查询Java字节码。这可以提高大型查询的性能，但是
codegen可以减缓非常简短的查询。
*   spark.sql.inMemoryColumnarStorage.com
pressed
    * 自动压缩内存中的存储的数据
*   spark.sql.inMemoryColumnarStorage.batch
Size
    *  缓存大小。需要考虑内存容量
*   spark.sql.parquet.compression.codec
    *   压缩编解码器，包括 uncompressed, snappy,gzip, and lzo.

