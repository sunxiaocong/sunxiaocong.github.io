---
layout: post
title: "Googel机器学习阅读笔记(2)"
date: 2019-01-27
categories: 机器学习
tags: [机器学习,TensorFlow]
image: http://gastonsanchez.com/https://github.com/daijiangtian/NoteBook/blob/master/机器学习/时间序列/https://github.com/daijiangtian/NoteBook/blob/master/机器学习/时间序列/https://github.com/daijiangtian/NoteBook/blob/master/机器学习/时间序列/https://github.com/daijiangtian/NoteBook/blob/master/机器学习/GOOGLE机器学习/https://github.com/daijiangtian/NoteBook/blob/master/机器学习/GOOGLE机器学习/images/blog/mathjax_logo.png?raw=true?raw=true?raw=true?raw=true?raw=true
---

Googel机器学习阅读笔记
Google视频地址：https://developers.google.cn/machine-learning/crash-course/

<!-- more -->
### 降低损失
在机器学习的过程中每一轮训练都有着对应的误差，我们的目标是将误差降到最低
。如下图所示，输入的特征经过模型预测后的结果与实际的标签存在则误差，因此我们
需要将这一误差反馈给模型让下一轮的计算损失降低、也就是参数更新。
![](https://github.com/daijiangtian/NoteBook/blob/master/机器学习/GOOGLE机器学习/images/机器学习过程.png?raw=true)
我们可以简化表述这个问题：
* y~ = w0 + w1 x1 
* 我们可以随意的选择初始值 w0 = 0 ,w1 = 0 
* 假设 x1 = 10 ，y = 1 
* 此时 y~ = 0 + 0 *（10） = 0 
* 这里我们使用平方损失作为损失函数那么损失为 y^2 - y~^2 = 1
* 最后根据这一损失函数来更新 w0 w1 的值一直迭代直到计算损失不在下降 
       
   
在上所说的参数更新可以抽象成一个数学问题在一个矢量空间 W 中 每一个w对应这一个损失值，
现在我们要求得就是最小损失时的权重值。如图:
![](https://github.com/daijiangtian/NoteBook/blob/master/机器学习/GOOGLE机器学习/images/权重损失图.png?raw=true)
* 对于凸形问题只有一个最低点，那么在斜率为0时损失函数就收敛了。
* 就像我们在爬山时想最快的走到最高点不会一圈一圈的走上，而是之间从山脊线走过去一样
我们想找到这一最低值也不会把空间中所有的 W 都计算一边他的损失值。
* 梯度下降法和他是一个道理。梯度下降法会计算损失曲线的梯度。梯度是偏导数的矢量，他始终指向损失函数增长最迅猛的方向
那么接下里只要往负梯度方向更新。
    
梯度是一个矢量他有着方向与大小，在朝着负梯度方向更新的过程中，还有一个每一次走多少的标量 学习率。
学习率的大小决定了每一次参数更新的浮动大小。如图所示：
![](https://github.com/daijiangtian/NoteBook/blob/master/机器学习/GOOGLE机器学习/images/学习率.png?raw=true)
* 学习率过大 那么可能一下走到了另一头，甚至下一次的损失值比现在还大
* 学习率过小，那么在找到最小值之前会有非常多的计算，而且在非凸平面上容易陷入局部最小。
* 在面对海量样时，如果我们对每一个样本都进行一次梯度的更新，那么计算量将太过庞大，因此我们会选择随机选择一些进行梯度的计算来
代替整体的梯度，“随机”这一术语表示构成各个批量的一个样本都是随机选择的（SGD随机梯度算法）。
* 小批量随机梯度下降法（小批量 SGD）是介于全批量迭代与 SGD 之间的折衷方案。小批量通常包含 10-1000 个随机选择的样本。小批量 SGD 可以减少 SGD 中的杂乱样本数量，但仍然比全批量更高效。

### 泛化

####  拟合
如在一个平面上有分布这红色和黄色的点 我们尝试用一条线将不同颜色的点分布在线的两边
这一过程为拟合
![](https://github.com/daijiangtian/NoteBook/blob/master/机器学习/GOOGLE机器学习/images/拟合.png?raw=true)
####  过拟合
如使用一条非常弯曲的线将点分在先的两边。但是当我们再有新的数据需要预测时，他将不能够很好的进行预测
![](https://github.com/daijiangtian/NoteBook/blob/master/机器学习/GOOGLE机器学习/images/过拟合.png?raw=true)
####  正则化
*   降低模型复杂度
*   以测试集的误差为准提前停止
*   添加模型复杂度惩罚项
    
#### L2正则化
* 以最小化损失和复杂度为目标，这称为结构风险最小化
* 一个是损失项，用于衡量模型与数据的拟合度，另一个是正则化项，用于衡量模型复杂度。
    * 将模型复杂度作为模型中所有特征的权重的函数。
        * 特征权重的绝对值越高，对模型复杂度的贡献就越大。
        *  L2 正则化公式来量化复杂度，该公式将正则化项定义为所有特征权重的平方和
            *   在执行了L2正则化后 
            *   使权重值接近于 0（但并非正好为 0）
            *   使权重的平均值接近于 0，且呈正态（钟形曲线或高斯曲线）分布
    * 将模型复杂度作为具有非零权重的特征总数的函数。
    
* 当一个模型中 正则化项的系数越高 可能模型会太简单  太低模型会太复杂
#### 奥卡姆剃刀定律
* 模型越简单，良好的证实结果就越有可能不仅仅基于样本的特性。
* 因此我们在构建模型的时候，应该从简单的角度入手，避免在训练过程中刚开始就出现过拟合。  
    
####  机器学习细则
* 我们从分布中随机抽取独立同分布 (i.i.d) 的样本。换言之，样本之间不会互相影响。（另一种解释：i.i.d. 是表示变量随机性的一种方式）。
* 分布是平稳的；即分布在数据集内不会发生变化。
* 我们从同一分布的数据划分中抽取样本    