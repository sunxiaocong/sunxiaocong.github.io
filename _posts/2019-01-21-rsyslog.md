---
layout: post
title: "Rsyslog 配置"
date: 2019-02-16
categories: 大数据
tags: [数据采集,rsyslog]
image: http://gastonsanchez.com/images/blog/mathjax_logo.png
---

ryslog 是一个快速处理收集系统日志的程序，提供了高性能、安全功能和模块化设计。rsyslog 是syslog 的升级版，它将多种来源输入输出转换结果到目的地，据官网介绍，现在可以处理100万条信息。

<!-- more -->

### rsyslog to flume 
```shell
module( load="imtcp" )
input( type="imtcp" port="514" ruleset="forwardruleset" )
Ruleset( name="forwardruleset" )
{
    action (
        type="omfwd"
        Target="$b-server-ip"
        Port="514"
        Protocol="tcp"
        RebindInterval="5000"
        name="action_fwd"
        queue.filename="action_fwd"
        queue.size="50000"
        queue.dequeuebatchsize="1000"
        queue.maxdiskspace="5G"
        queue.discardseverity="3"
        queue.checkpointinterval="10"
        queue.type="linkedlist"
        queue.workerthreads="1"
        queue.timeoutshutdown="10"
        queue.timeoutactioncompletion="10"
        queue.timeoutenqueue="20"
        queue.timeoutworkerthreadshutdown="10"
        queue.workerthreadminimummessages="5000"
        queue.maxfilesize="500M"
        queue.saveonshutdown="on"
    )
    stop
}

```

### rsyslog to elasticsearch  

```shell
$MaxMessageSize 10m            # 设置最大收发日志大小(字节)

# 模块载入，非内置模块需提前安装

module(load="imptcp")          # 载入TCP模块，同imtcp，但针对Linux做了高性能定制
module(load="imuxsock")        # 载入unixsocket模块，旧收集方案使用到这一个接口，保留
module(load="builtin:omfile")  # 载入文件输出模块
module(load="mmjsonparse")     # 载入json解析模块
module(load="omelasticsearch") # 载入Elasticsearch输出模块
module(load="impstats" interval="10" facility="5" severity="6" log.syslog="on" format="json" resetCounters="on") # 载入监控Rsyslog性能模块

# 输入源模块

input(type="imptcp" port="514")  # 通过TCP/514端口接收日志
input(type="imuxsock" Socket="/dev/unixlog/log.sock" CreatePath="on") # 通过unixsocket接收日志

# 日志模板

# 监控模块Elasticsearch索引名模板，形如：impstats-2018.07.11
template(name="impstatsTemplate" type="list"){
        constant(value="impstats-")
        property(name="$year")
        constant(value=".")
        property(name="$month")
        constant(value=".")
        property(name="$day")
}

# 备份文件名模板，形如：/data/log/dq2_chat/dq2_chat.2018-07-11.log
template(name="backFileName" 
         type="string" 
         string="/data/log/%$!@fluentd_tag%/%$!@fluentd_tag%.%$year%-%$month%-%$day%.log")

# json解析失败记录模板，形如：/data/run/parsing_failure.2018-07-11.log
template(name="parseErrFileName" 
         type="string" 
         string="/data/run/parsing_failure.%$year%-%$month%-%$day%.log")

# 日志Elasticsearch索引名模板，形如：dq2_chat-2018.07.11
template(name="indexTemplate" type="list"){
        property(name="$!@fluentd_tag")
        constant(value="-")
        property(name="$year")
        constant(value=".")
        property(name="$month")
        constant(value=".")
        property(name="$day")
}

# impstats日志格式模板，形如：{"@timestamp""："2018-07-11T04:08:01.003197+00:00", "hostname":"192.168.1.1","name":"global","origin": "dynstats","values": {}}
template(name="impstatsLogFormat" type="list") {
         constant(value="{ ")
         constant(value="\"@timestamp\":\"")
         property(name="timegenerated" dateFormat="rfc3339" )
         constant(value="\", ")
         constant(value="\"hostname\":\"")
         property(name="hostname")
         constant(value="\", ")
         property(name="$!all-json" position.from="2")
         constant(value="\n")
}

# json解析成功的日志格式模板，形如：{"@timestamp""："2018-07-11T04:08:01.003197+00:00", "hostname": "192.168.1.1", "func_name": "test", "line_no": "67"}
template(name="logFormat" type="list") {
         constant(value="{ ")
         constant(value="\"@timestamp\":\"")
         property(name="timegenerated" dateFormat="rfc3339" )
         constant(value="\", ")
         constant(value="\"hostname\":\"")
         property(name="hostname")
         constant(value="\", ")
         property(name="$!all-json" position.from="2")
         constant(value="\n")
}

# json解析失败的日志直接全部放入log字段
template(name="parseFailLogFormat" type="list" option.jsonf="on") {
         property(outname="@timestamp" name="timereported" dateFormat="rfc3339" format="jsonf")
         property(outname="hostname" name="hostname" format="jsonf")
         property(outname="severity" name="syslogseverity-text" caseConversion="upper" format="jsonf")
         property(outname="log" name="msg" format="jsonf")
}

# 主队列配置

main_queue(
    queue.type="linkedlist"        # 队列类型为linkedlist，使用内存作为缓冲，且动态分配大小
    queue.size="2000000"           # 主队列容量
    queue.dequeuebatchsize="10000" # 批量处理的日志数
    queue.workerthreads="2"        # 主队列工作线程数
)

# 日志分发操作

# 如果是impstats的日志
if $syslogfacility-text == 'syslog' then {
    # 进行json解析 
    action(name="parse_impstats"
          cookie=""
          type="mmjsonparse"
    )
    # 发送到Elasticsearch
    action(type="omelasticsearch" 
          server=["http://XXXX:9200","http://10.30.25.41:9200","http://xxxx:9200"]
          template="impstatsLogFormat"
          dynSearchIndex="on"
          searchIndex="impstatsTemplate"
          searchType="rsyslog"
          bulkmode="on" 
          maxbytes="100m"
          queue.type="linkedlist" 
          queue.size="400000" 
          queue.dequeuebatchsize="10000"
          queue.workerthreads="5"
          action.resumeretrycount="-1"
          errorFile="/data/run/impstats.log")
}
# 如果是SDK传过来的日志
if $syslogfacility-text == 'local0' then 
{
    # 进行json解析
    action(type="mmjsonparse" cookie="")
    if $parsesuccess == "OK" then {
        # 如果解析成功，将解析结果发送到Elasticsearch
        action(type="omelasticsearch" 
                server=["http://XXXX:9200","http://XXXX:9200","http://XXXX:9200"] # Elasticsearch主节点列表
                template="logFormat"           # 指定日志格式模板
                dynSearchIndex="on"            # 使用动态索引名
                searchIndex="indexTemplate"    # 指定索引名模板
                searchType="fluentd"           # _type，兼容考虑，这里设为fluentd
                bulkmode="on"                  # 使用Elasticsearch批量接口
                maxbytes="100m"                # 批量接口一次请求最大字节数，不要超过Elasticsearch本身的http.max_content_length设置（默认100m）
                queue.type="linkedlist"        # 分发操作队列类型
                queue.size="400000"            # 队列大小
                queue.dequeuebatchsize="10000" # 批量处理的日志数
                queue.workerthreads="5"        # 工作线程数
                action.resumeretrycount="-1"   # 如果因Elasticsearch没有响应，无限次重试发送，防止丢弃
                errorFile="/data/run/omes_error.log") # 请求出错信息记录
        # 备份除连接池、会话、orm外的日志
        if (not ($!@fluentd_tag contains "frontend")) and (not ($!@fluentd_tag contains "session")) and (not ($!@fluentd_tag contains "orm")) then {
            action(type="omfile" 
                    Template="logFormat"    # 指定存入文件的日志格式模板
                    ioBufferSize="128K"     # IO缓冲大小
                    dirCreateMode="0700"    # 创建的目录的权限
                    FileCreateMode="0644"   # 创建的文件的权限
                    dynaFile="backFileName" # 使用的动态文件名的模板
                    asyncWriting="on"       # 通过独立线程异步写文件，将使用2个缓冲区，一个存，一个写 
                    flushInterval="1")      # 清空缓冲区间隔
        }
    }
    # 如果解析失败，将日志放入log字段，也发送到Elasticsearch
    else if $parsesuccess == "FAIL" then {
        action(type="omfile" 
                dynaFile="parseErrFileName")
        action(type="omelasticsearch" 
                server=["http://XXXX:9200","http://XXXX:9200","http://XXXX:9200"]
                template="parseFailLogFormat" 
                dynSearchIndex="on"
                searchIndex="indexTemplate"
                searchType="fluentd"
                bulkmode="on" 
                maxbytes="100m" 
                queue.type="linkedlist" 
                queue.size="400000" 
                queue.dequeuebatchsize="10000"
                queue.workerthreads="5"
                action.resumeretrycount="-1"
                errorFile="/data/run/omes_error.log")
        if (not ($!@fluentd_tag contains "frontend")) and (not ($!@fluentd_tag contains "session")) and (not ($!@fluentd_tag contains "orm")) then {
            action(type="omfile" 
                    Template="parseFailLogFormat" 
                    ioBufferSize="128K" 
                    dirCreateMode="0700" 
                    FileCreateMode="0644"
                    dynaFile="backFileName"
                    asyncWriting="on"
                    flushInterval="1")
        }
    }

}
```

