---
layout: post
title: "Googel机器学习阅读笔记(1)"
date: 2019-01-27
categories: 机器学习
tags: [机器学习,TensorFlow]
image: http://gastonsanchez.com/https://github.com/daijiangtian/NoteBook/blob/master/机器学习/时间序列/https://github.com/daijiangtian/NoteBook/blob/master/机器学习/时间序列/https://github.com/daijiangtian/NoteBook/blob/master/机器学习/时间序列/https://github.com/daijiangtian/NoteBook/blob/master/机器学习/GOOGLE机器学习/images/blog/mathjax_logo.png?raw=true?raw=true?raw=true?raw=true
---

Googel机器学习阅读笔记
Google视频地址：https://developers.google.cn/machine-learning/crash-course/

<!-- more -->

### 机器学习主要术语

#### 标签 
标签是对我们要预测事物的结果，即坐标中的y轴。

#### 特征 
特征是我们要预测的事物具有的特性，若单一特性可作为x轴。
    
#### 样本
* 无标签样本：X。指数据的特定实例 X (X为一个各个特征x1,x2,x3...构成的矢量)
* 有标签样本 （X，Y）。
    
#### 模型 
模型定义了特征与标签的关系（预测值由模型内部参数定义，这些内部参数的值是通过学习得到的） 
* 模型的训练： 在大量有标签样本下，让模型不断的学习特征与标签的关系
* 模型的推断： 即用无标签样本让模型进行预测。

#### 分类与回归
* 分类用于将离散的事物分开，预测该事物属于哪一类
* 回归用于连续发生的事情，预测他接来的状态。

### 回归

####  线性回归
* 线性关系
![图一](https://github.com/daijiangtian/NoteBook/blob/master/机器学习/GOOGLE机器学习/images/回归图.png?raw=true)
    * 如图当我们想要刻画途中横纵坐标的关系时，我们会画出一条直线使点均匀的落在直线的附近。
    用简单的代数来刻画则是构造函数： y = mx + b   
    * 在机器学习中横坐标x 往往是一个矢量 X（x1,x2,x3...）对应的m也应该是一个矢量这里用w（w1,w2,w3）表示权重。对应的b 
    在一些文档中为了方便表示将b 记做 w0 ，x0为1 则 Y = W X 
* 训练与损失
![图二](https://github.com/daijiangtian/NoteBook/blob/master/机器学习/GOOGLE机器学习/images/训练与损失图.png?raw=true)
    * 训练模型是通过有标签的样本来学习所有权重和偏差的理想值。训练的目的是为了让
    模型的预测的结果与实际情况越来越接近。预测的结果又是由权重 W 确定。
    * 损失这是对模型预测结果的一种评估。若预测完全正确则损失为零，否则损失会比较大。
    损失的刻画有非常多中 如常见的平方损失：
    
    ![图](https://github.com/daijiangtian/NoteBook/blob/master/机器学习/GOOGLE机器学习/images/平方d损失.png?raw=true)
  
    * (x,y)指的是样本，其中
        * x是指模型进行预测时使用的特征集
        * y是指样本表情
    * prediction(x) 指的是权重的偏差与特征集(X)结合的函数
    * D 是指包含多个有标签样本的数据集
    * N 指的是D中的样本数量

#### 逻辑回归
* 需要一种跟合适的损失函数与预测方法 
* 逻辑回归
    * 将概率解析为0到1的值 
        * S型函数  y = 1/(1+e^(-z))  
        * 其中z为线性输出， z为 对几几率 
    * 使用对数损失函数
        ![](https://github.com/daijiangtian/NoteBook/blob/master/机器学习/GOOGLE机器学习/images/对数损失函数.png?raw=true)
        * (xy)ϵD 是包含很多有标签样本 (x,y) 的数据集。
        * “y”是有标签样本中的标签。由于这是逻辑回归，因此“y”的每个值必须是 0 或 1。
        * “y'”是对于特征集“x”的预测值（介于 0 和 1 之间）。
        * 对数损失函数的方程式与 Shannon 信息论中的熵测量密切相关。它也是似然函数的负对数（假设“y”属于伯努利分布）。实际上，最大限度地降低损失函数的值会生成最大的似然估计值。
        
    
            