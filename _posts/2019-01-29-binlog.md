---
layout: post
title: "基于上游数据库日志同步采集"
date: 2019-01-22
categories: 大数据
tags: [数据采集,Python]
image: http://gastonsanchez.com/images/blog/mathjax_logo.png
---

在日志的上下游需要中转时如果采用开启一个http服务用请求的方式当设计的表多了管理起来就非常麻烦
因此这里采用直接对数据库访问，采集实时的数据本文包含了　mysql　binlog　采集　mongo　oplog　采集
postgres　wal　采集　

Github地址:https://github.com/daijiangtian/db-transfer

<!-- more -->

### MYSQL

binlog日志用于记录所有更新了数据或者已经潜在更新了数据（例如，没有匹配任何行的一个DELETE）的所有语句。语句以“事件”的形式保存，它描述数据更改。

因为有了数据更新的binlog，所以可以用于实时备份，与master/slave主从复制结合。

#### Binlog操作

```
binlog的删除可以手工删除或自动删除：
a）自动删除binlog
通过binlog参数（expire_logs_days ）来实现mysql自动删除binlog
mysql> show binary logs;
mysql> show variables like 'expire_logs_days';      //该参数表示binlog日志自动删除/过期的天数，默认值为0，表示不自动删除
mysql> set global expire_logs_days=3;        //表示日志保留3天，3天后就自动过期。
b）手工删除binlog
mysql> reset master;        //删除master的binlog，即手动删除所有的binlog日志
mysql> reset slave;          //删除slave的中继日志
mysql> purge master logs before '2012-03-30 17:20:00';         //删除指定日期以前的日志索引中binlog日志文件
mysql> purge master logs to 'binlog.000002';       //删除指定日志文件的日志索引中binlog日志文件

mysql> set sql_log_bin=1/0;       //如果用户有super权限，可以启用或禁用当前会话的binlog记录
mysql> show master logs;          //查看master的binlog日志列表
mysql> show binary logs;           //查看master的binlog日志文件大小
mysql> show master status;     //用于提供master二进制日志文件的状态信息
mysql> show slave hosts;        //显示当前注册的slave的列表。不以--report-host=slave_name选项为开头的slave不会显示在本列表中

mysql> flush logs;     //产生一个新的binlog日志文件

```

#### Binlog日志格式

Mysql binlog日志有三种格式，分别是Statement、MiXED、ROW

##### Statement

每一条会修改数据的sql都会记录在binlog中
优点：不需要记录每一行的变化，减少了binlog日志量，节约了IO，提高性能。(相比row能节约多少性能与日志量，这个取决于应用的SQL情况，正常同一条记录修改或者插入row格式所产生的日志量还小于Statement产生的日志量，但是考虑到如果带条件的update操作，以及整表删除，alter表等操作，ROW格式会产生大量日志，因此在考虑是否使用ROW格式日志时应该跟据应用的实际情况，其所产生的日志量会增加多少，以及带来的IO性能问题。)
缺点：由于记录的只是执行语句，为了这些语句能在slave上正确运行，因此还必须记录每条语句在执行的时候的一些相关信息，以保证所有语句能在slave得到和在master端执行时候相同 的结果。另外mysql 的复制,像一些特定函数功能，slave可与master上要保持一致会有很多相关问题(如sleep()函数， last_insert_id()，以及user-defined functions(udf)会出现问题).

使用以下函数的语句也无法被复制：
* LOAD_FILE()
* UUID()
* USER()
* FOUND_ROWS()
* SYSDATE() (除非启动时启用了 --sysdate-is-now 选项)

同时在INSERT ...SELECT 会产生比 RBR 更多的行级锁

##### Row

不记录sql语句上下文相关信息，仅保存哪条记录被修改
优点： binlog中可以不记录执行的sql语句的上下文相关的信息，仅需要记录那一条记录被修改成什么了。所以rowlevel的日志内容会非常清楚的记录下每一行数据修改的细节。而且不会出现某些特定情况下的存储过程，或function，以及trigger的调用和触发无法被正确复制的问题
缺点:所有的执行的语句当记录到日志中的时候，都将以每行记录的修改来记录，这样可能会产生大量的日志内容,比如一条update语句，修改多条记录，则binlog中每一条修改都会有记录，这样造成binlog日志量会很大，特别是当执行alter table之类的语句的时候，由于表结构修改，每条记录都发生改变，那么该表每一条记录都会记录到日志中。

##### Mixedlevel

是以上两种level的混合使用，一般的语句修改使用statment格式保存binlog，如一些函数，statement无法完成主从复制的操作，则采用row格式保存binlog,MySQL会根据执行的每一条具体的sql语句来区分对待记录的日志形式，也就是在Statement和Row之间选择一种.新版本的MySQL中队row level模式也被做了优化，并不是所有的修改都会以row level来记录，像遇到表结构变更的时候就会以statement模式来记录。至于update或者delete等修改数据的语句，还是会记录所有行的变更。

Mixed日志说明：
在slave日志同步过程中，对于使用now这样的时间函数，MIXED日志格式，会在日志中产生对应的unix_timestamp()*1000的时间字符串，slave在完成同步时，取用的是sqlEvent发生的时间来保证数据的准确性。另外对于一些功能性函数slave能完成相应的数据同步，而对于上面指定的一些类似于UDF函数，导致Slave无法知晓的情况，则会采用ROW格式存储这些Binlog，以保证产生的Binlog可以供Slave完成数据同步。


#### 核心代码
```python
#!/usr/bin/env python
"""
 Created by Dai at 19-1-28.
"""

import time

from pymysqlreplication import BinLogStreamReader
from pymysqlreplication.row_event import (
    DeleteRowsEvent,
    UpdateRowsEvent,
    WriteRowsEvent,
)

from src.config import Config
from src.database.mysql import mysql


class Reader:

    def __init__(self):
        self.mysql_settings = Config.zk_config().get("mysql_settings")
        try:
            self.start_file = Config.zk_config()["start_file"]
            self.start_pos = Config.zk_config()["start_pos"]
        except:
            self.start_file, self.start_pos = mysql.fetchone("SHOW MASTER STATUS")[:2]
            data = {
                "start_file": self.start_file,
                "start_pos": self.start_pos,
                "mysql_settings": self.mysql_settings
            }
            Config.update(data)

    def get_data(self):

        flag = 0
        stream = None
        while True:
            mysql_settings = Config.zk_config().get("mysql_settings")
            start_file = Config.zk_config()["start_file"]
            start_pos = Config.zk_config()["start_pos"]

            try:
                if flag == 0:
                    stream = BinLogStreamReader(
                        connection_settings=mysql_settings,
                        server_id=101,
                        blocking=False,
                        only_schemas=None,
                        only_events=[DeleteRowsEvent, WriteRowsEvent, UpdateRowsEvent],
                        resume_stream=True,
                        log_file=start_file, log_pos=start_pos)
                    flag = 1

                for binlogevent in stream:
                    for row in binlogevent.rows:
                        event = {"schema": binlogevent.schema, "table": binlogevent.table,
                                 "log_pos": binlogevent.packet.log_pos}
                        if isinstance(binlogevent, DeleteRowsEvent):
                            event["action"] = "delete"
                            event["values"] = dict(row["values"].items())
                            event = dict(event.items())
                        elif isinstance(binlogevent, UpdateRowsEvent):
                            event["action"] = "update"
                            event["before_values"] = dict(row["before_values"].items())
                            event["after_values"] = dict(row["after_values"].items())
                            event = dict(event.items())
                        elif isinstance(binlogevent, WriteRowsEvent):
                            event["action"] = "insert"
                            event["values"] = dict(row["values"].items())
                            event = dict(event.items())
                        start_pos = event['log_pos']

                        yield event


                else:
                    file, pos = mysql.fetchone("SHOW MASTER STATUS")[:2]
                    if file != start_file:
                        start_file = file
                        start_pos = 4
                        stream.close()
                        flag = 0
                    data = {
                        "start_file": start_file,
                        "start_pos": start_pos,
                        "mysql_settings": self.mysql_settings
                    }

                    Config.update(data)

                    time.sleep(2)
            except Exception as e:
                print(e)
                stream.close()
                flag = 0


if __name__ == "__main__":
    read = Reader()
    res = read.get_data()
    print(res.__next__())
    print(res.__next__())
    print(res.__next__())
    print(res.__next__())
```


### MONGO

oplog是local库下的一个固定集合，Secondary就是通过查看Primary 的oplog这个集合来进行复制的。每个节点都有oplog，记录这从主节点复制过来的信息，这样每个成员都可以作为同步源给其他节点。
Oplog 可以说是Mongodb Replication的纽带了。

#### OPLOG数据结构

```
    db.oplog.rs.find().skip(1).limit(1).toArray()
```

- ts: 8字节的时间戳，由4字节unix timestamp + 4字节自增计数表示。这个值很重要，在选举(如master宕机时)新primary时，会选择ts最大的那个secondary作为新primary
- op：1字节的操作类型
- "i"： insert
- "u"： update
- "d"： delete
- "c"： db cmd
- "db"：声明当前数据库 (其中ns 被设置成为=>数据库名称+ '.')
- "n": no op,即空操作，其会定期执行以确保时效性
- ns：操作所在的namespace
- o：操作所对应的document，即当前操作的内容（比如更新操作时要更新的的字段和值）
- o2: 在执行更新操作时的where条件，仅限于update时才有该属性

#### 查看oplog　日志信息

```
    db.printReplicationInfo()
```

- configured oplog size： oplog文件大小
- log length start to end: oplog日志的启用时间段
- oplog first event time: 第一个事务日志的产生时间
- oplog last event time: 最后一个事务日志的产生时间
- now: 现在的时间

#### 核心代码
```python
#!/usr/bin/env python
"""
 Created by Dai at 19-3-19.
"""

from time import sleep

from pymongo import ASCENDING
from pymongo.cursor import CursorType
from pymongo.errors import AutoReconnect
from src.config import Config
from src.database.mongo import mongo_db

from bson.timestamp import Timestamp
# Tailable cursor options.
_TAIL_OPTS = {'tailable': True, 'await_data': True}

# Time to wait for data or connection.
_SLEEP = 10



class Read():

    def __init__(self):

        self.oplog = mongo_db.get_col("local", "oplog.rs")

        data = Config.zk_config()

        try :
            self.stamp = Config.zk_config()["stamp"]
            self.stamp = Timestamp(time=self.stamp['_Timestamp__time'],inc=self.stamp['_Timestamp__inc'])
        except:
            self.stamp = self.oplog.find().sort('$natural', ASCENDING).limit(-1).next()['ts']
        data['stamp'] = self.stamp.__dict__

        Config.update(data)

    def get_data(self):

        while True:
            kw = {}

            kw['filter'] = {'ts': {'$gt': self.stamp}}
            kw['cursor_type'] = CursorType.TAILABLE_AWAIT
            kw['oplog_replay'] = True

            cursor = self.oplog.find(**kw)

            try:
                while cursor.alive:
                    for doc in cursor:
                        self.stamp = doc['ts']
                        yield doc

                    data = Config.zk_config()
                    data['stamp'] = self.stamp.__dict__
                    Config.update()
                    sleep(_SLEEP)



            except AutoReconnect:
                data = Config.zk_config()
                data['stamp'] = self.stamp.__dict__
                Config.update()
                sleep(_SLEEP)


if __name__ == '__main__':
    mongo_read = Read()

    res = mongo_read.get_data()

    import time

    print(res.__next__())
```

### POSTGRES

WAL即Write-Ahead Logging,预写式日志（WAL）是保证数据完整性的一种标准方法。WAL的中心概念是数据文件（存储着表和索引）的修改必须在这些动作被日志记录之后才被写入，即在描述这些改变的日志记录被刷到持久存储以后。如果我们遵循这种过程，我们不需要在每个事务提交时刷写数据页面到磁盘，因为我们知道在发生崩溃时可以使用日志来恢复数据库：任何还没有被应用到数据页面的改变可以根据其日志记录重做（这是前滚恢复，也被称为REDO）。WAL 的中心思想是先写日志，再写数据，数据文件的修改必须发生在这些修改已经记录在日志文件中之后。

#### WAL过程

- Write-Ahead Logging，前写日志。
- PostgreSQL的存储结构：元组-文件页-物理段-表；以及写数据的步骤：先写到缓冲区Buffer-再刷新到磁盘Disk。WAL机制实际是在这个写数据的过程中加入了对应的写wal log的过程，步骤一样是先到Buffer，再刷新到Disk。

- Change发生时：先将变更后内容记入WAL Buffer 再将更新后的数据写入Data Buffer
- Commit发生时：WAL Buffer刷新到Disk Data Buffer写磁盘推迟
- Checkpoint发生时：将所有Data Buffer刷新到磁盘

#### 配置
在下列文件添加配置

pb_hdb.conf
```
    local replication testrepl trust
```

postgresql.conf
```
```
    wal_level = logical
    max_wal_senders = 3
    max_replication_slots = 3
```

#### 核心代码
```python
#!/usr/bin/env python
"""
 Created by Dai at 19-3-19.
"""
from datetime import datetime
from select import select

import psycopg2
from psycopg2.extras import LogicalReplicationConnection

from src.config import Config


class Read():

    def __init__(self):
        self.postgres_settings = Config.zk_config().get("postgres_settings")

        self.conn = psycopg2.connect(
            connection_factory=LogicalReplicationConnection, **self.postgres_settings)
        self.cur = self.conn.cursor()

        try:
            self.cur.create_replication_slot('pytest2', output_plugin='test_decoding')
        except:
            pass

        self.start_lsn = Config.zk_config().get("start_lsn", 0)

        self.cur.start_replication(slot_name='pytest2', decode=True, start_lsn=self.start_lsn)
        # cur.consume_stream(consume)

    def get_data(self):

        keepalive_interval = 10.0
        while True:
            msg = self.cur.read_message()
            if msg:

                self.start_lsn = msg.data_start
                yield msg
            else:

                now = datetime.now()
                timeout = keepalive_interval - (now - self.cur.io_timestamp).total_seconds()
                try:
                    sel = select([self.cur], [], [], max(0, timeout))
                    if not any(sel):
                        data = Config.zk_config()
                        data['start_lsn'] = self.start_lsn
                        Config.update(data)
                        self.cur.send_feedback()  # timed out, send keepalive message
                except InterruptedError:

                    data = Config.zk_config()
                    data['start_lsn'] = self.start_lsn
                    Config.update(data)
                    pass  # recalculate timeout and continue


if __name__ == '__main__':
    read = Read()

    res = read.get_data()

    for i in res:
        print(i.payload)

```