---
layout: post
title: "日志采集方案(上)"
date: 2019-01-23
categories: 大数据
tags: [ElasticSearch,Flume,Rsyslog,HDFS,数据采集]
image: http://gastonsanchez.com/images/blog/mathjax_logo.png
---

本文为面向的业务场景为不考虑数据少量丢失，高实时，高吞吐量的采集方案
数据的目的地有 ElasticSearch 和 HDFS 其中ElasticSearch 提供实时的搜索
HDFS提供大量数据的计算 

<!-- more -->

## flume-hdfs-sink 参数配置
### 日志收集流程图
![image](http://assets.processon.com/chart_image/5bf2205ce4b0e9468c2905e0.png?_=1542960393748)
### 服务端日志格式
* syslog消息格式为<pri>gamename_logname date,other partition key:content 
```
<131>dq2_var_log 2018-11-19,1:{"filename": "test.py", "message": "test message!", "log_time": "2018-11-23 14:32:39", "trace_id": 123456}
```
* 两条日志之间通过换行符分割  
* json日志内容中，key，value必须使用双引号，value值为int类型不需要双引号。空值为null或者不传值，不可传None。
* ps:需要往es上发送的日志必须是json格式 
* 文件滚动规则，正在写的日志文件的后缀为.tmp，在适当大小滚动文件，将后缀改为.log,.log在采集完成后会被删除掉，采集过程中不予许对.log文件进行写入和修改。


### rsyslog配置
```
# 根据放置在hostname位置的index值生成索引
template(name="indexTemplate" type="list") {
    property(name="hostname")
     constant(value="-")
     property(name="$year")
     constant(value=".")
     property(name="$month")
     constant(value=".")
     property(name="$day")

}
# 按照日志原本的格式将日志转发到flume上的模板
template(name="flume-proxy" type="string" 
        string="<%PRI%>%hostname% %syslogtag%%msg%\n"
    )

@@ flume_hostname:port
```


### 服务端日志收集flume配置
```
###syslog采集端
a1.sources.r1.type = syslogtcp
a1.sources.r1.host = 0.0.0.0
a1.sources.r1.port = 5141
a1.sources.r1.eventSize = 4096
a1.sources.r1.interceptors.i1.type = regex_extractor
a1.sources.r1.interceptors.i1.regex = >([\\w-]*)\s([\\w-]*),([\\w-]*):
a1.sources.r1.interceptors.i1.serializers = s1 s2 s3
a1.sources.r1.interceptors.i1.serializers.s1.name = log_name
a1.sources.r1.interceptors.i1.serializers.s2.name = log_date
a1.sources.r1.interceptors.i1.serializers.s3.name = server

a1.sources.r1.interceptors.i2.type = search_replace
a1.sources.r1.interceptors.i2.searchPattern = ^<[\\w->,]*:
a1.sources.r1.interceptors.i2.replaceString = 


#使用选择流将日志进行分发
a1.sources.r1.selector.type = multiplexing  
a1.sources.r1.selector.header = State  
a1.sources.r1.selector.mapping.coin_log = c1  c11
a1.sources.r1.selector.mapping.var_log = c2  c21
a1.sources.r1.selector.mapping.login_log = c3  c31


### elasticsearch-sink配置，将日志传输到elasticsearch
a4.sinks.k1.type = myelasticsearch.ElasticSearchSink //使用定制es-sink
a4.sinks.k1.es.client.hosts = 192.168.5.45:9300,192.168.5.46:9300,192.168.5.47:9300
a4.sinks.k1.es.cluster.name = local_efk //es集群名称
a4.sinks.k1.es.index.builder = myelasticsearch.HeaderIndexBuilder
a4.sinks.k1.es.headerIndex.name = log_name //索引标题
a4.sinks.k1.channel = c1 


#hdfs sink 配置  
a4.sinks.k2.channel = c2
a4.sinks.k2.type = hdfs
a4.sinks.k2.hdfs.path = hdfs://10.30.25.210:9000/flume/%{log_name}/log_date=%{log_date}/server=%{server}/
a4.sinks.k2.hdfs.fileType = SequenceFile
a4.sinks.k2.hdfs.codeC = gzip  //使用gzip进行文件压缩
a4.sinks.k2.hdfs.writeFormat = Text 
a4.sinks.k2.hdfs.rollSize = 125829120  //文件滚动大小
a4.sinks.k2.hdfs.rollCount = 0 //文件滚动日志行数
a4.sinks.k2.hdfs.rollInterval = 3600 //文件滚动时间
a4.sinks.k2.hdfs.batchSize = 10000  //批量大小
a4.sinks.k2.hdfs.minBlockReplicas = 1 //hdfs同步份数
```


### hive 建立外部表读取flume上传文件
* 创建外部表
```
create external table 表名(   
字段名 类型，  //字段名与类型要与json匹配，字段可以多或者少
)  
PARTITIONED BY (字段名，类型)
ROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe'  
STORED AS sequencefile  
location '/path/to/your/table'; 
```

* 添加分区

```
由于使用直接写hdfs的方式存储数据，hive的matestore无法找到这些数据，需要手动为数据增加分区
MSCK REPAIR TABLE table_name;

执行msdk将会检查表目录下的所有文件并为没有分区的目录创建分区，执行时间由分区数据决定，分区数越多执行时间越长
ALTER TABLE table_name ADD PARTITION (字段=值) location '/flume/dq2_var_log/2018-11-21/1'

分区数需要与路径上的值对应，否则读取是会出错
```

* 删除分区

```
* 如果出于某些原因导致删除了hdfs上的文件，这时如果不将对应的分区删除的话在读取数据的就会报错，这时就需要手动删除相应的分区。
ALTER TABLE table_name DROP PARTITION (字段=值)
删除分区只能一个一个的删除，如果需要大量的删除分区的话建议使用脚本删除或后者现将表删除然后重建表，并用msck重建分区
```

#### 参考资料源
#hive建表相关配置  
http://bigdata.51cto.com/art/201706/542623.htm  
https://blog.csdn.net/helloxiaozhe/article/details/78465467  
#flume-hdfs-sink 参数官方文档  
http://flume.apache.org/FlumeUserGuide.html#hdfs-sink  
