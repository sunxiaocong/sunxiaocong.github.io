---
layout: post
title: "Tensorflow 神经网络"
date: 2019-01-29
categories: 机器学习
tags: [机器学习,TensorFlow,Python]
image: http://gastonsanchez.com/https://github.com/daijiangtian/NoteBook/blob/master/机器学习/时间序列/https://github.com/daijiangtian/NoteBook/blob/master/机器学习/时间序列/https://github.com/daijiangtian/NoteBook/blob/master/机器学习/时间序列/https://github.com/daijiangtian/NoteBook/blob/master/机器学习/GOOGLE机器学习/https://github.com/daijiangtian/NoteBook/blob/master/机器学习/GOOGLE机器学习/images/blog/mathjax_logo.png?raw=true?raw=true?raw=true?raw=true?raw=true
---

* 提取问题中实体特征向量作为神经网络的输入

* 定义神经网络的结构，并定义如何从神经网络的输入到输出．

<!-- more -->

### 前向传播算法

#### 神经元　
* 一个神经元有多个输入和一个输出
* 神经网络结构是指不同神经元之间的链接
* 最简单的神经元结构的输出　等于　所有输入的加权和
* 相邻两层任意两个节点之间都有链接．全连接（一个链接对应一参数）

#### 前向传播过程

三部分信息

* 神经网络的输入

* 神经网络链接结构

* 每个神经元的参数

    * 初始化随机参数
    
    ```
    //生成均值为０　方差为２的随机数　可以用mean指定平均值　没指定默认为０
    weights = tf.Variavle(tf.random_normal([2,3],stddev=2))	
    ```

    * 随机数生成函数
    
    ```
    //正太分布
    tf.random_normal
    //正太分布，如果随机踹的值离平均值超过两个标准差　那么将被重新随机
    truncated_normal　
    //均匀分布
    tf.random_uniform
    //Gamma分布
    tf.random_gamma
    ```

    * 常数初始化
    
    ```
    tf.zeros([2,3],int32)
    
    tf.ones([2,3],int32)
    //全部根据给定值填充
    tf.fill([2,3],9)
    //定值常量
    tf.constant([1,2,3])
    ```

    * 整个过程可以理解成矩阵的乘法
    
    ```
    tf.matmul(x.y)
    ```

#### 神经网络实例

```
#!/usr/bin/env python
"""
 Created by Dai at 18-9-28.
"""

import tensorflow as tf
# 利用numpy生成模拟数据集
from numpy.random import RandomState

batch_size = 8

# 设置seed随机种子　保证每次运行是一样的

w1 = tf.Variable(tf.random_normal([2, 3], stddev=1, seed=1))
w2 = tf.Variable(tf.random_normal([3, 1], stddev=1, seed=1))

# 定义placeholder作为存储数据的地方
x = tf.placeholder(tf.float32, shape=(None, 2), name="input")
y_ = tf.placeholder(tf.float32, shape=(None, 1), name="input")

# 前向传播
a = tf.matmul(x, w1)
y = tf.matmul(a, w2)

# 定义损失函数和反向传播
# tf.clip_by_value　可以将值限制在某一个范围内
# *　表示点乘　而不是矩阵想乘法
cross_entropy = -tf.reduce_mean(y_ * tf.log(tf.clip_by_value(y, 1e-10, 1.0)))

train_step = tf.train.AdamOptimizer(0.001).minimize(cross_entropy)

rdm = RandomState(1)
dataset_size = 128

X = rdm.rand(dataset_size, 2)

Y = [[int(x1 + x2 < 1)] for (x1, x2) in X]

with tf.Session() as session:
    init_op = tf.initialize_all_variables()
    session.run(init_op)

    print(session.run(w1))
    print(session.run(w2))

    STEPS = 5000

    for i in range(STEPS):
        start = (i * batch_size) % dataset_size
        end = min(start + batch_size, dataset_size)

        session.run(train_step, feed_dict={x: X[start:end], y_: Y[start:end]})

    print(session.run(w1))
    print(session.run(w2))
```

### 深层神经网络

#### 深度学习

一类通过多层非线性变化对高复杂性数据建模算法的集合

#### 线性模型的局限
* 任意线性模型组合仍旧是线性模型
* 线性模型最终是通过一个超平面来划分，因此只能解决线性可分问题

#### 构造非线性

激活函数

* 线性函数的输出为一个非线性函数　那么模型则变为非线性

常用激活函数
* ReLU
* sigmoid
* tanh

使用
```
a = tf.nn.relu(tf.matmul(x,w1)+biases1)
y = tf.nn.relu(tf.matmul(a,w2)+biases2)
```

偏置项
* 每个节点的取值不单纯是加权求和

Softmax

* 将输出转换为概率分布

损失函数
* 交叉熵（cross entropy）　用于分类问题
```
cross_entropy = -tf.reduce_mean(y_ * tf.log(tf.clip_by_value(y, 1e-10, 1.0)))
```

* 均方误差(MSE) 　用于回归问题

```
mse = tf.reduce_mean(tf.square(y_-y))
```

* 自定义损失函数

```
tf.greater　比较每一个元素的大小，并且返回比较结果
tf.select 根据选择条件　对第二个或者第三个参数进行计算
loss = tf.reduce_sum(tf.select(tf.greater(v1,v2),(v1-v2)*a,(v2-v1)*b)
```

#### 梯度下降算法
* 迭代更新参数不断沿着梯度反方向让参数朝着总损失更小的放心更新
* 学习率
    * 每一次参数更新的幅度
    * 学习率更新　指数衰减　tf.trainxexponential_decay
        * dacayed_learing_rate = learing_reate * decay_reate ^ (global_step / decay_steps)
        * decay_rate 衰减系数
        * decay_steps　衰减速度

#### 反向传播算法

* 更好的对全部参数使用梯度下降算法
* learning representations by back-propagating errors

#### 正则化

* 用于防止过拟合

* 在算是函数中加入正则化函数　对参数做出限制

```
# lambda 表示正则化中的权重
loss = tf.reduce_mean(tf.square(y_-y))+tf.contrib.layers.l2_regularizer(lambda)(w1)
```

#### 滑动平均模型

对参数设置一个副本　用于平滑的更新参数

```
v1 = tf.Varibale(0,dtype=tf.float32)

step = tf.Variable(0,traninale=False)
# 定义一滑动平均类
ema  = tf.train.ExponentiaMovingAverage(0.99,step)

maintain_averages_op = ema.apply([v1])
#v1　的值更新为５
session.run(tf.assign(v1,5))
print(session.run([v1,ema.average(v1)]))
#输出 [5, 4.5]

```

            