---
layout: post
title: "Tensorflow 神经网络"
date: 2019-01-29
categories: 机器学习
tags: [机器学习,TensorFlow,Python]
image: http://gastonsanchez.com/https://github.com/daijiangtian/NoteBook/blob/master/机器学习/时间序列/https://github.com/daijiangtian/NoteBook/blob/master/机器学习/时间序列/https://github.com/daijiangtian/NoteBook/blob/master/机器学习/时间序列/https://github.com/daijiangtian/NoteBook/blob/master/机器学习/GOOGLE机器学习/https://github.com/daijiangtian/NoteBook/blob/master/机器学习/GOOGLE机器学习/images/blog/mathjax_logo.png?raw=true?raw=true?raw=true?raw=true?raw=true
---

* 提取问题中实体特征向量作为神经网络的输入

* 定义神经网络的结构，并定义如何从神经网络的输入到输出．

<!-- more -->

### 前向传播算法

#### 神经元　
* 一个神经元有多个输入和一个输出
* 神经网络结构是指不同神经元之间的链接
* 最简单的神经元结构的输出　等于　所有输入的加权和
* 相邻两层任意两个节点之间都有链接．全连接（一个链接对应一参数）

#### 前向传播过程

三部分信息

* 神经网络的输入

* 神经网络链接结构

* 每个神经元的参数

    * 初始化随机参数
    
    ```
    //生成均值为０　方差为２的随机数　可以用mean指定平均值　没指定默认为０
    weights = tf.Variavle(tf.random_normal([2,3],stddev=2))	
    ```

    * 随机数生成函数
    
    ```
    //正太分布
    tf.random_normal
    //正太分布，如果随机踹的值离平均值超过两个标准差　那么将被重新随机
    truncated_normal　
    //均匀分布
    tf.random_uniform
    //Gamma分布
    tf.random_gamma
    ```

    * 常数初始化
    
    ```
    tf.zeros([2,3],int32)
    
    tf.ones([2,3],int32)
    //全部根据给定值填充
    tf.fill([2,3],9)
    //定值常量
    tf.constant([1,2,3])
    ```

    * 整个过程可以理解成矩阵的乘法
    
    ```
    tf.matmul(x.y)
    ```

#### 神经网络实例

```
#!/usr/bin/env python
"""
 Created by Dai at 18-9-28.
"""

import tensorflow as tf
# 利用numpy生成模拟数据集
from numpy.random import RandomState

batch_size = 8

# 设置seed随机种子　保证每次运行是一样的

w1 = tf.Variable(tf.random_normal([2, 3], stddev=1, seed=1))
w2 = tf.Variable(tf.random_normal([3, 1], stddev=1, seed=1))

# 定义placeholder作为存储数据的地方
x = tf.placeholder(tf.float32, shape=(None, 2), name="input")
y_ = tf.placeholder(tf.float32, shape=(None, 1), name="input")

# 前向传播
a = tf.matmul(x, w1)
y = tf.matmul(a, w2)

# 定义损失函数和反向传播
# tf.clip_by_value　可以将值限制在某一个范围内
# *　表示点乘　而不是矩阵想乘法
cross_entropy = -tf.reduce_mean(y_ * tf.log(tf.clip_by_value(y, 1e-10, 1.0)))

train_step = tf.train.AdamOptimizer(0.001).minimize(cross_entropy)

rdm = RandomState(1)
dataset_size = 128

X = rdm.rand(dataset_size, 2)

Y = [[int(x1 + x2 < 1)] for (x1, x2) in X]

with tf.Session() as session:
    init_op = tf.initialize_all_variables()
    session.run(init_op)

    print(session.run(w1))
    print(session.run(w2))

    STEPS = 5000

    for i in range(STEPS):
        start = (i * batch_size) % dataset_size
        end = min(start + batch_size, dataset_size)

        session.run(train_step, feed_dict={x: X[start:end], y_: Y[start:end]})

    print(session.run(w1))
    print(session.run(w2))
```

        