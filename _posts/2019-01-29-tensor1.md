---
layout: post
title: "Tensorflow 神经网络"
date: 2019-01-29
categories: 机器学习
tags: [机器学习,TensorFlow,Python]
image: http://gastonsanchez.com/https://github.com/daijiangtian/NoteBook/blob/master/机器学习/时间序列/https://github.com/daijiangtian/NoteBook/blob/master/机器学习/时间序列/https://github.com/daijiangtian/NoteBook/blob/master/机器学习/时间序列/https://github.com/daijiangtian/NoteBook/blob/master/机器学习/GOOGLE机器学习/https://github.com/daijiangtian/NoteBook/blob/master/机器学习/GOOGLE机器学习/images/blog/mathjax_logo.png?raw=true?raw=true?raw=true?raw=true?raw=true
---

* 提取问题中实体特征向量作为神经网络的输入

* 定义神经网络的结构，并定义如何从神经网络的输入到输出．

<!-- more -->

### 前向传播算法

#### 神经元　
* 一个神经元有多个输入和一个输出
* 神经网络结构是指不同神经元之间的链接
* 最简单的神经元结构的输出　等于　所有输入的加权和
* 相邻两层任意两个节点之间都有链接．全连接（一个链接对应一参数）

#### 前向传播过程

三部分信息

* 神经网络的输入

* 神经网络链接结构

* 每个神经元的参数

    * 初始化随机参数
    
    ```
    //生成均值为０　方差为２的随机数　可以用mean指定平均值　没指定默认为０
    weights = tf.Variavle(tf.random_normal([2,3],stddev=2))	
    ```

    * 随机数生成函数
    
    ```
    //正太分布
    tf.random_normal
    //正太分布，如果随机踹的值离平均值超过两个标准差　那么将被重新随机
    truncated_normal　
    //均匀分布
    tf.random_uniform
    //Gamma分布
    tf.random_gamma
    ```

    * 常数初始化
    
    ```
    tf.zeros([2,3],int32)
    
    tf.ones([2,3],int32)
    //全部根据给定值填充
    tf.fill([2,3],9)
    //定值常量
    tf.constant([1,2,3])
    ```

    * 整个过程可以理解成矩阵的乘法
    
    ```
    tf.matmul(x.y)
    ```

#### 神经网络实例

```
#!/usr/bin/env python
"""
 Created by Dai at 18-9-28.
"""

import tensorflow as tf
# 利用numpy生成模拟数据集
from numpy.random import RandomState

batch_size = 8

# 设置seed随机种子　保证每次运行是一样的

w1 = tf.Variable(tf.random_normal([2, 3], stddev=1, seed=1))
w2 = tf.Variable(tf.random_normal([3, 1], stddev=1, seed=1))

# 定义placeholder作为存储数据的地方
x = tf.placeholder(tf.float32, shape=(None, 2), name="input")
y_ = tf.placeholder(tf.float32, shape=(None, 1), name="input")

# 前向传播
a = tf.matmul(x, w1)
y = tf.matmul(a, w2)

# 定义损失函数和反向传播
# tf.clip_by_value　可以将值限制在某一个范围内
# *　表示点乘　而不是矩阵想乘法
cross_entropy = -tf.reduce_mean(y_ * tf.log(tf.clip_by_value(y, 1e-10, 1.0)))

train_step = tf.train.AdamOptimizer(0.001).minimize(cross_entropy)

rdm = RandomState(1)
dataset_size = 128

X = rdm.rand(dataset_size, 2)

Y = [[int(x1 + x2 < 1)] for (x1, x2) in X]

with tf.Session() as session:
    init_op = tf.initialize_all_variables()
    session.run(init_op)

    print(session.run(w1))
    print(session.run(w2))

    STEPS = 5000

    for i in range(STEPS):
        start = (i * batch_size) % dataset_size
        end = min(start + batch_size, dataset_size)

        session.run(train_step, feed_dict={x: X[start:end], y_: Y[start:end]})

    print(session.run(w1))
    print(session.run(w2))
```

### 深层神经网络

#### 深度学习

一类通过多层非线性变化对高复杂性数据建模算法的集合

#### 线性模型的局限
* 任意线性模型组合仍旧是线性模型
* 线性模型最终是通过一个超平面来划分，因此只能解决线性可分问题

#### 构造非线性

激活函数

* 线性函数的输出为一个非线性函数　那么模型则变为非线性

常用激活函数
* ReLU
* sigmoid
* tanh

使用
```
a = tf.nn.relu(tf.matmul(x,w1)+biases1)
y = tf.nn.relu(tf.matmul(a,w2)+biases2)
```

偏置项
* 每个节点的取值不单纯是加权求和

Softmax

* 将输出转换为概率分布

损失函数
* 交叉熵（cross entropy）　用于分类问题
```
cross_entropy = -tf.reduce_mean(y_ * tf.log(tf.clip_by_value(y, 1e-10, 1.0)))
```

* 均方误差(MSE) 　用于回归问题

```
mse = tf.reduce_mean(tf.square(y_-y))
```

* 自定义损失函数

```
tf.greater　比较每一个元素的大小，并且返回比较结果
tf.select 根据选择条件　对第二个或者第三个参数进行计算
loss = tf.reduce_sum(tf.select(tf.greater(v1,v2),(v1-v2)*a,(v2-v1)*b)
```

#### 梯度下降算法
* 迭代更新参数不断沿着梯度反方向让参数朝着总损失更小的放心更新
* 学习率
    * 每一次参数更新的幅度
    * 学习率更新　指数衰减　tf.trainxexponential_decay
        * dacayed_learing_rate = learing_reate * decay_reate ^ (global_step / decay_steps)
        * decay_rate 衰减系数
        * decay_steps　衰减速度

#### 反向传播算法

* 更好的对全部参数使用梯度下降算法
* learning representations by back-propagating errors

#### 正则化

* 用于防止过拟合

* 在算是函数中加入正则化函数　对参数做出限制

```
# lambda 表示正则化中的权重
loss = tf.reduce_mean(tf.square(y_-y))+tf.contrib.layers.l2_regularizer(lambda)(w1)
```

#### 滑动平均模型

对参数设置一个副本　用于平滑的更新参数

```
v1 = tf.Varibale(0,dtype=tf.float32)

step = tf.Variable(0,traninale=False)
# 定义一滑动平均类
ema  = tf.train.ExponentiaMovingAverage(0.99,step)

maintain_averages_op = ema.apply([v1])
#v1　的值更新为５
session.run(tf.assign(v1,5))
print(session.run([v1,ema.average(v1)]))
#输出 [5, 4.5]

```
### MNIST
```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
# @Time    : 2018/10/7/007 10:55
# @Author  : Woe
# @Site    : 
# @File    : 04MNIST.py
# @Software: PyCharm

import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data

# 设置相关常数
INPUT_NODE = 784
OUTPUT_NODE = 10

# 配置神经网络参数
LAYER1_NODE = 500  # 隐藏层节点数

BATCH_SIZE = 100

LEARNING_RATE_BASE = 0.8
LEARNING_RATE_DECAY = 0.99

REGULARIZATION_RATE = 0.0001  # 模型复杂度正则化项在损失函数中的系数
TRAINING_STEPS = 30000

MOVING_AVERAGE_DECAY = 0.99  # 滑动平均衰减率


def inference(input_tensor, avg_class, weightsl, biases1, weights2, biases2):
    """
        一个辅助函数，给定神经网络的输入和所有参数，计算神经网络的向前传播结果
        在这里定义一个使用ReLU激活函数的三层全连接神经网络。用过加入隐藏层时间多层网络结构，
        方便在测试时使用滑动平均模型
    :param input_tensor: 
    :param avg_class: 
    :param weightsl: 
    :param biasesl: 
    :param weights2: 
    :param biases2: 
    :return: 
    """
    if avg_class == None:
        # 计算隐藏层的向前传播结果
        layer1 = tf.nn.relu(tf.matmul(input_tensor, weightsl) + biases1)

        # 计算输出程的向前传播结果。引文计算损失函数时会一并计算softmax函数 所有这里不需要加入softmax
        return tf.matmul(layer1, weights2) + biases2

    else:
        # 首先使用 avg_class.average 函数计算出变量滑动平均值
        layer1 = tf.nn.relu(tf.matmul(input_tensor, avg_class.average(weightsl)) + avg_class.average(biases1))
        return tf.matmul(layer1, avg_class.average(weights2)) + avg_class.average(biases2)


def train(mnist):
    """
        训练数据
    :param mnist:
    :return:
    """
    x = tf.placeholder(tf.float32, [None, INPUT_NODE], name="x-input")
    y_ = tf.placeholder(tf.float32, [None, OUTPUT_NODE], name="y-input")

    # 隐藏层参数
    weights1 = tf.Variable(tf.truncated_normal([INPUT_NODE, LAYER1_NODE], stddev=0.1))
    biases1 = tf.Variable(tf.constant(0.1, shape=[LAYER1_NODE]))

    # 输出层参数
    weights2 = tf.Variable(tf.truncated_normal([LAYER1_NODE, OUTPUT_NODE], stddev=0.1))
    biases2 = tf.Variable(tf.constant(0.1, shape=[OUTPUT_NODE]))

    y = inference(x, None, weights1, biases1, weights2, biases2)

    # 定义训练轮数变量，可以加快训练早期变量的更新速度
    # trainable 表示是否可以用来训练
    global_step = tf.Variable(0, trainable=False)

    # 给定滑动平均模型的衰减率和训练轮数
    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)

    variable_averages_op = variable_averages.apply(tf.trainable_variables())

    average_y = inference(x, variable_averages, weights1, biases1, weights2, biases2)

    # 当问题只有一个正确答案时候 使用 sparse_softmax_cross_entropy_with_logits 来加速交叉熵计算

    # 标准答案是一个 长度为10的数组 该函数需要提供一个正确答案的数字 需要用 tf.argmax 来得到对应的编号

    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))
    # 计算当前所有的平均值
    cross_entropy_mean = tf.reduce_mean(cross_entropy)

    # 计算L2正则化损失函数
    regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)
    regularization = regularizer(weights1) + regularizer(weights2)

    loss = cross_entropy_mean + regularization

    # 设置衰减学习率
    learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE, global_step, mnist.train.num_examples / BATCH_SIZE,
                                               LEARNING_RATE_DECAY)

    # 使用 优化算法来优化损失函数
    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)

    # 在训练神经网络为了一次完成多个操作 使用 tf.control_dependencies 和 tf.group
    # train_op = tf.group(train_step,variables_averages_op)
    with tf.control_dependencies([train_step, variable_averages_op]):
        train_op = tf.no_op(name="train")

    correct_prediction = tf.equal(tf.arg_max(average_y, 1), tf.arg_max(y_, 1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

    with tf.Session() as sess:
        tf.initialize_all_variables().run()

        # 准备验证数据， 用于判断停止

        validate_feed = {
            x: mnist.validation.images,
            y_: mnist.validation.labels
        }

        # c测试数据

        test_feed = {
            x: mnist.test.images,
            y_: mnist.test.labels
        }

        for i in range(TRAINING_STEPS):
            # 每1000 轮输出一次数据集结果
            if i % 1000 == 0:
                validate_acc = sess.run(accuracy, feed_dict=validate_feed)
                print(f"after {i} training steps validate accuracy using acerage model is {validate_acc}")
                # 训练结束
                test_acc = sess.run(accuracy, feed_dict=test_feed)

                print(f"after {TRAINING_STEPS} test steps test accuracy using acerage model is {test_acc}")

            xs, ys = mnist.train.next_batch(BATCH_SIZE)

            sess.run(train_op, feed_dict={x: xs, y_: ys})


def main(argv=None):
    # 数据在本地先下载好 可以直接使用
    mnist = input_data.read_data_sets("./MNIST_data", one_hot=True)
    train(mnist)


if __name__ == '__main__':
    tf.app.run()

```
            