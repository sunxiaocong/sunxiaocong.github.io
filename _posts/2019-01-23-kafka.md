---
layout: post
title: "日志采集方案(下)"
date: 2019-01-23
categories: 大数据
tags: [Kafka,Flume,数据采集]
image: http://gastonsanchez.com/images/blog/mathjax_logo.png
---

本文为面向的业务场景为需要在服务的上游和下游之间做一个缓冲，来面对高请求下上游服务不会受到
下游的影响，该方案采用Kafka作为中间件 服务上游对接Flume 将数据发往Kafka 服务下游通过编写消费者
将数据入库。 

<!-- more -->

## Flume
### Flume配置
```yaml
# list the sources, sinks and channels in the a1
a1.sources = r1 
a1.sinks = k1 
a1.channels = c1

a1.sources.r1.type = thrift
a1.sources.r1.channels = c1
a1.sources.r1.bind = 0.0.0.0
a1.sources.r1.port = 57888
a1.sources.r1.threads = 200

# channel selector configuration
a1.sources.r1.selector.type = multiplexing
a1.sources.r1.selector.header = State
a1.sources.r1.selector.mapping.coin_log = c1

a1.channels.c1.type = file
a1.channels.c1.dataDirs = /data/a1/a1c1-dq2/flume-data
a1.channels.c1.useDualCheckpoints=true
a1.channels.c1.checkpointDir = /data/a1/a1c1-dq2/flume-checkpoint1
a1.channels.c1.backupCheckpointDir = /data/a1/a1c1-dq2/flume-checkpoint2
a1.channels.c1.capacity = 250000
a1.channels.c1.transactionCapacity = 28000
a1.channels.c1.maxFileSize=52428800
a1.channels.c1.checkpointInterval=30000

a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink
a1.sinks.k1.topic = dq2_coin_log
a1.sinks.k1.brokerList = 10.46.142.117:9092,10.46.20.156:9092,10.46.248.21:9092
a1.sinks.k1.requiredAcks = -1
a1.sinks.k1.kafka.producer.batch.size= 1000
a1.sinks.k1.channel = c1
a1.sinks.k1.kafka.producer.max.request.size = 6000000

```

### Flume SDK
编写SDK一般要满足三方面的要求：
- 数据不可丢失，对应失败重传机制
- 数据可以批量发送
- 数据可以缓存，缓解Flume压力  

在配置中设置了数据源是 thrift 其中Flume有各个版本的thrift接口只需将其放在自己的项目里即可使用

## Kafka

### Kafka配置

传输过程中主要使用JSON为序列化方式
配合FlumeSDK需要返回状态码

### Kafka消费者
Kafka一般有两种消费模式：
- 下拉数据后在马上就提交分片，然后处理数据 (自动提交)
- 下拉数据等待数据全部处理完毕后，在处理数据 (手动提交)
无论使用哪种模式，都应当保证数据的一致性，与完备性。

#### 自动提交

在Python中由于无法手动commit，因此在使用Python编写消费者时，需要有一套机制来保证从Kafka拉下的数据
在完全入库之前不会丢失.我们使用的是写文件的形式将数据落地，以防服务挂了机器重启等原因丢失了数据

在这里我们定义了3种文件：
- 待入库文件
- 重传文件
- 失败文件

整个数据的走向为 

- Kafka 下拉数据 去重后写入待入库文件
- 重传文件写入待入库文件，删除重传文件
- 做一些业务操作
    - 如查询：如果是大量的查询 使用IN 批量查找后在做业务逻辑，性能会远高于单条单条查询
- 判断数据批发还是单发，若入库失败判断失败类型是字段错误还是数据库异常
    - 批发
        - 失败需要从TrackBack中将异常的那条记录的行号取出来 放入异常文件，然后将数据剩下的数据写入重试文件
    - 单发
        - 判断异常状况写入对应文件即可
        
#### 手动提交

由于是业务处理完后再提交因此没有了数据丢失的问题 只需要做好去重的工作即可