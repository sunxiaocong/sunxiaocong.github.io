---
layout: post
title: "日志采集方案(下)"
date: 2019-01-23
categories: 大数据
tags: [Kafka,Flume,数据采集]
image: http://gastonsanchez.com/images/blog/mathjax_logo.png
---

本文为面向的业务场景为需要在服务的上游和下游之间做一个缓冲，来面对高请求下上游服务不会受到
下游的影响，该方案采用Kafka作为中间件 服务上游对接Flume 将数据发往Kafka 服务下游通过编写消费者
将数据入库。 

<!-- more -->

## Flume
### Flume配置
```yaml
# list the sources, sinks and channels in the a1
a1.sources = r1 
a1.sinks = k1 
a1.channels = c1

a1.sources.r1.type = thrift
a1.sources.r1.channels = c1
a1.sources.r1.bind = 0.0.0.0
a1.sources.r1.port = 57888
a1.sources.r1.threads = 200

# channel selector configuration
a1.sources.r1.selector.type = multiplexing
a1.sources.r1.selector.header = State
a1.sources.r1.selector.mapping.coin_log = c1

a1.channels.c1.type = file
a1.channels.c1.dataDirs = /data/a1/a1c1-dq2/flume-data
a1.channels.c1.useDualCheckpoints=true
a1.channels.c1.checkpointDir = /data/a1/a1c1-dq2/flume-checkpoint1
a1.channels.c1.backupCheckpointDir = /data/a1/a1c1-dq2/flume-checkpoint2
a1.channels.c1.capacity = 250000
a1.channels.c1.transactionCapacity = 28000
a1.channels.c1.maxFileSize=52428800
a1.channels.c1.checkpointInterval=30000

a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink
a1.sinks.k1.topic = dq2_coin_log
a1.sinks.k1.brokerList = 10.46.142.117:9092,10.46.20.156:9092,10.46.248.21:9092
a1.sinks.k1.requiredAcks = -1
a1.sinks.k1.kafka.producer.batch.size= 1000
a1.sinks.k1.channel = c1
a1.sinks.k1.kafka.producer.max.request.size = 6000000

```

### Flume SDK
编写SDK一般要满足三方面的要求：
- 数据不可丢失，对应失败重传机制
- 数据可以批量发送
- 数据可以缓存，缓解Flume压力  

在配置中设置了数据源是 thrift 其中Flume有各个版本的thrift接口只需将其放在自己的项目里即可使用

## Kafka

### Kafka配置

传输过程中主要使用JSON为序列化方式
配合FlumeSDK需要返回状态码

### Kafka消费者
Kafka一般有两种消费模式：
- 下拉数据后在马上就提交分片，然后处理数据 (自动提交)
- 下拉数据等待数据全部处理完毕后，在处理数据 (手动提交)
无论使用哪种模式，都应当保证数据的一致性，与完备性。

#### 自动提交

在Python中由于无法手动commit，因此在使用Python编写消费者时，需要有一套机制来保证从Kafka拉下的数据
在完全入库之前不会丢失.我们使用的是写文件的形式将数据落地，以防服务挂了机器重启等原因丢失了数据

在这里我们定义了3种文件：
- 待入库文件
- 重传文件
- 失败文件

整个数据的走向为 

- Kafka 下拉数据 去重后写入待入库文件
- 重传文件写入待入库文件，删除重传文件
- 做一些业务操作
    - 如查询：如果是大量的查询 使用IN 批量查找后在做业务逻辑，性能会远高于单条单条查询
- 判断数据批发还是单发，若入库失败判断失败类型是字段错误还是数据库异常
    - 批发
        - 失败需要从TrackBack中将异常的那条记录的行号取出来 放入异常文件，然后将数据剩下的数据写入重试文件
    - 单发
        - 判断异常状况写入对应文件即可
        
#### 手动提交

由于是业务处理完后再提交因此没有了数据丢失的问题 只需要做好去重的工作即可





```
###syslog采集端
a1.sources.r1.type = syslogtcp
a1.sources.r1.host = 0.0.0.0
a1.sources.r1.port = 5141
a1.sources.r1.eventSize = 4096
a1.sources.r1.interceptors.i1.type = regex_extractor
a1.sources.r1.interceptors.i1.regex = >([\\w-]*)\s([\\w-]*),([\\w-]*):
a1.sources.r1.interceptors.i1.serializers = s1 s2 s3
a1.sources.r1.interceptors.i1.serializers.s1.name = log_name
a1.sources.r1.interceptors.i1.serializers.s2.name = log_date
a1.sources.r1.interceptors.i1.serializers.s3.name = server

a1.sources.r1.interceptors.i2.type = search_replace
a1.sources.r1.interceptors.i2.searchPattern = ^<[\\w->,]*:
a1.sources.r1.interceptors.i2.replaceString = 


#使用选择流将日志进行分发
a1.sources.r1.selector.type = multiplexing  
a1.sources.r1.selector.header = State  
a1.sources.r1.selector.mapping.coin_log = c1  c11
a1.sources.r1.selector.mapping.var_log = c2  c21
a1.sources.r1.selector.mapping.login_log = c3  c31


### elasticsearch-sink配置，将日志传输到elasticsearch
a4.sinks.k1.type = myelasticsearch.ElasticSearchSink //使用定制es-sink
a4.sinks.k1.es.client.hosts = 192.168.5.45:9300,192.168.5.46:9300,192.168.5.47:9300
a4.sinks.k1.es.cluster.name = local_efk //es集群名称
a4.sinks.k1.es.index.builder = myelasticsearch.HeaderIndexBuilder
a4.sinks.k1.es.headerIndex.name = log_name //索引标题
a4.sinks.k1.channel = c1 


#hdfs sink 配置  
a4.sinks.k2.channel = c2
a4.sinks.k2.type = hdfs
a4.sinks.k2.hdfs.path = hdfs://10.30.25.210:9000/flume/%{log_name}/log_date=%{log_date}/server=%{server}/
a4.sinks.k2.hdfs.fileType = SequenceFile
a4.sinks.k2.hdfs.codeC = gzip  //使用gzip进行文件压缩
a4.sinks.k2.hdfs.writeFormat = Text 
a4.sinks.k2.hdfs.rollSize = 125829120  //文件滚动大小
a4.sinks.k2.hdfs.rollCount = 0 //文件滚动日志行数
a4.sinks.k2.hdfs.rollInterval = 3600 //文件滚动时间
a4.sinks.k2.hdfs.batchSize = 10000  //批量大小
a4.sinks.k2.hdfs.minBlockReplicas = 1 //hdfs同步份数
```


### hive 建立外部表读取flume上传文件
* 创建外部表
```
create external table 表名(   
字段名 类型，  //字段名与类型要与json匹配，字段可以多或者少
)  
PARTITIONED BY (字段名，类型)
ROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe'  
STORED AS sequencefile  
location '/path/to/your/table'; 
```

* 添加分区

```
* 由于使用直接写hdfs的方式存储数据，hive的matestore无法找到这些数据，需要手动为数据增加分区
MSCK REPAIR TABLE table_name;
* 执行msdk将会检查表目录下的所有文件并为没有分区的目录创建分区，执行时间由分区数据决定，分区数越多执行时间越长
ALTER TABLE table_name ADD PARTITION (字段=值) location '/flume/dq2_var_log/2018-11-21/1'
* 分区数需要与路径上的值对应，否则读取是会出错
```

* 删除分区

```
* 如果出于某些原因导致删除了hdfs上的文件，这时如果不将对应的分区删除的话在读取数据的就会报错，这时就需要手动删除相应的分区。
ALTER TABLE table_name DROP PARTITION (字段=值)
删除分区只能一个一个的删除，如果需要大量的删除分区的话建议使用脚本删除或后者现将表删除然后重建表，并用msck重建分区
```

#### 参考资料源
#hive建表相关配置  
http://bigdata.51cto.com/art/201706/542623.htm  
https://blog.csdn.net/helloxiaozhe/article/details/78465467  
#flume-hdfs-sink 参数官方文档  
http://flume.apache.org/FlumeUserGuide.html#hdfs-sink  
